@online{231209031IComMa,
  title = {[2312.09031] {{iComMa}}: {{Inverting 3D Gaussian Splatting}} for {{Camera Pose Estimation}} via {{Comparing}} and {{Matching}}},
  url = {https://arxiv.org/abs/2312.09031},
  urldate = {2024-07-10},
  file = {C:\Users\18317\Zotero\storage\BZ2U62AZ\2312.html}
}

@software{AddonItem,
  title = {Addon {{Item}}},
  keywords = {nosource}
}

@software{AddonItema,
  title = {Addon {{Item}}},
  keywords = {nosource}
}

@article{bescosDynaSLAMTrackingMapping2018,
  title = {{{DynaSLAM}}: {{Tracking}}, Mapping, and Inpainting in Dynamic Scenes},
  shorttitle = {{{DynaSLAM}}},
  author = {Bescos, Berta and Fácil, José M. and Civera, Javier and Neira, José},
  date = {2018},
  journaltitle = {IEEE Robotics and Automation Letters},
  volume = {3},
  number = {4},
  pages = {4076--4083},
  publisher = {IEEE},
  doi = {10.1109/LRA.2018.2860039},
  url = {https://ieeexplore.ieee.org/abstract/document/8421015/},
  urldate = {2024-09-26},
  file = {C:\Users\18317\DevSpace\DocHub\essay\zotero\attachments\gs_spalt\relate_works\Bescos et al_2018_DynaSLAM.pdf}
}

@inproceedings{beslMethodRegistration3shapes1992,
  title = {Method for Registration of 3-{{D}} Shapes},
  booktitle = {Sensor Fusion {{IV}}: Control Paradigms and Data Structures},
  author = {Besl, Paul J. and McKay, Neil D.},
  date = {1992},
  volume = {1611},
  pages = {586--606},
  publisher = {Spie},
  doi = {10.1117/12.57955},
  url = {https://www.spiedigitallibrary.org/conference-proceedings-of-spie/1611/1/Method-for-registration-of-3-D-shapes/10.1117/12.57955.short},
  urldate = {2024-09-13},
  file = {C:\Users\18317\DevSpace\DocHub\essay\zotero\attachments\gs_spalt\relate_works\Besl_McKay_1992_Method for registration of 3-D shapes.pdf}
}

@article{cadenaPresentFutureSimultaneous2016,
  title = {Past, Present, and Future of Simultaneous Localization and Mapping: {{Toward}} the Robust-Perception Age},
  shorttitle = {Past, Present, and Future of Simultaneous Localization and Mapping},
  author = {Cadena, Cesar and Carlone, Luca and Carrillo, Henry and Latif, Yasir and Scaramuzza, Davide and Neira, José and Reid, Ian and Leonard, John J.},
  date = {2016},
  journaltitle = {IEEE Transactions on robotics},
  volume = {32},
  number = {6},
  pages = {1309--1332},
  publisher = {IEEE},
  doi = {10.1109/TRO.2016.2624754},
  url = {https://ieeexplore.ieee.org/abstract/document/7747236/},
  urldate = {2024-09-26},
  file = {C:\Users\18317\DevSpace\DocHub\essay\zotero\attachments\gs_spalt\introduce\Cadena et al_2016_Past, present, and future of simultaneous localization and mapping.pdf}
}

@online{caiIkdTreeIncrementalKD2021,
  title = {Ikd-{{Tree}}: {{An Incremental K-D Tree}} for {{Robotic Applications}}},
  shorttitle = {Ikd-{{Tree}}},
  author = {Cai, Yixi and Xu, Wei and Zhang, Fu},
  date = {2021-02-22},
  eprint = {2102.10808},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2102.10808},
  url = {http://arxiv.org/abs/2102.10808},
  urldate = {2024-05-21},
  abstract = {This paper proposes an efficient data structure, ikd-Tree, for dynamic space partition. The ikd-Tree incrementally updates a k-d tree with new coming points only, leading to much lower computation time than existing static k-d trees. Besides point-wise operations, the ikd-Tree supports several features such as box-wise operations and down-sampling that are practically useful in robotic applications. In parallel to the incremental operations (i.e., insert, re-insert, and delete), ikd-Tree actively monitors the tree structure and partially re-balances the tree, which enables efficient nearest point search in later stages. The ikd-Tree is carefully engineered and supports multi-thread parallel computing to maximize the overall efficiency. We validate the ikd-Tree in both theory and practical experiments. On theory level, a complete time complexity analysis is presented to prove the high efficiency. On experiment level, the ikd-Tree is tested on both randomized datasets and real-world LiDAR point data in LiDAR-inertial odometry and mapping application. In all tests, ikd-Tree consumes only 4\% of the running time in a static k-d tree.},
  pubstate = {prepublished},
  keywords = {Computer Science - Robotics},
  file = {C\:\\Users\\18317\\DevSpace\\DocHub\\essay\\zotero\\attachments\\raw\\Cai et al_2021_ikd-Tree4.pdf;C\:\\Users\\18317\\Zotero\\storage\\LWWXKIMB\\2102.html}
}

@article{camposOrbslam3AccurateOpensource2021,
  title = {Orb-Slam3: {{An}} Accurate Open-Source Library for Visual, Visual–Inertial, and Multimap Slam},
  shorttitle = {Orb-Slam3},
  author = {Campos, Carlos and Elvira, Richard and Rodríguez, Juan J. Gómez and Montiel, José MM and Tardós, Juan D.},
  date = {2021},
  journaltitle = {IEEE Transactions on Robotics},
  volume = {37},
  number = {6},
  pages = {1874--1890},
  publisher = {IEEE},
  doi = {10.1109/TRO.2021.3075644},
  url = {https://ieeexplore.ieee.org/abstract/document/9440682/},
  urldate = {2024-08-20},
  file = {C:\Users\18317\DevSpace\DocHub\essay\zotero\attachments\gs_spalt\experiments\Campos et al_2021_Orb-slam3.pdf}
}

@article{chenIndoorCameraPose2022,
  title = {Indoor Camera Pose Estimation via Style‐transfer {{3D}} Models},
  author = {Chen, Junjie and Li, Shuai and Liu, Donghai and Lu, Weisheng},
  date = {2022-03},
  journaltitle = {Computer-Aided Civil and Infrastructure Engineering},
  shortjournal = {Computer aided Civil Eng},
  volume = {37},
  number = {3},
  pages = {335--353},
  issn = {1093-9687, 1467-8667},
  doi = {10.1111/mice.12714},
  url = {https://onlinelibrary.wiley.com/doi/10.1111/mice.12714},
  urldate = {2024-07-16},
  abstract = {Abstract             Many vision‐based indoor localization methods require tedious and comprehensive pre‐mapping of built environments. This research proposes a mapping‐free approach to estimating indoor camera poses based on a 3D style‐transferred building information model (BIM) and photogrammetry technique. To address the cross‐domain gap between virtual 3D models and real‐life photographs, a CycleGAN model was developed to transform BIM renderings into photorealistic images. A photogrammetry‐based algorithm was developed to estimate camera pose using the visual and spatial information extracted from the style‐transferred BIM. The experiments demonstrated the efficacy of CycleGAN in bridging the cross‐domain gap, which significantly improved performance in terms of image retrieval and feature correspondence detection. With the 3D coordinates retrieved from BIM, the proposed method can achieve near real‐time camera pose estimation with an accuracy of 1.38 m and 10.1° in indoor environments.},
  langid = {english},
  file = {C:\Users\18317\Zotero\storage\ULX2HNPM\Chen et al. - 2022 - Indoor camera pose estimation via style‐transfer 3.pdf}
}

@online{chenSurvey3DGaussian2024,
  title = {A {{Survey}} on {{3D Gaussian Splatting}}},
  author = {Chen, Guikun and Wang, Wenguan},
  date = {2024-07-22},
  eprint = {2401.03890},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2401.03890},
  urldate = {2024-09-20},
  abstract = {3D Gaussian splatting (GS) has recently emerged as a transformative technique in the realm of explicit radiance field and computer graphics. This innovative approach, characterized by the utilization of millions of learnable 3D Gaussians, represents a significant departure from mainstream neural radiance field approaches, which predominantly use implicit, coordinate-based models to map spatial coordinates to pixel values. 3D GS, with its explicit scene representation and differentiable rendering algorithm, not only promises real-time rendering capability but also introduces unprecedented levels of editability. This positions 3D GS as a potential game-changer for the next generation of 3D reconstruction and representation. In the present paper, we provide the first systematic overview of the recent developments and critical contributions in the domain of 3D GS. We begin with a detailed exploration of the underlying principles and the driving forces behind the emergence of 3D GS, laying the groundwork for understanding its significance. A focal point of our discussion is the practical applicability of 3D GS. By enabling unprecedented rendering speed, 3D GS opens up a plethora of applications, ranging from virtual reality to interactive media and beyond. This is complemented by a comparative analysis of leading 3D GS models, evaluated across various benchmark tasks to highlight their performance and practical utility. The survey concludes by identifying current challenges and suggesting potential avenues for future research in this domain. Through this survey, we aim to provide a valuable resource for both newcomers and seasoned researchers, fostering further exploration and advancement in applicable and explicit radiance field representation.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Graphics,Computer Science - Multimedia},
  file = {C\:\\Users\\18317\\DevSpace\\DocHub\\essay\\zotero\\attachments\\gs_spalt\\Chen_Wang_2024_A Survey on 3D Gaussian Splatting.pdf;C\:\\Users\\18317\\Zotero\\storage\\YTE8HDPC\\2401.html}
}

@article{choiIterativeKclosestPoint2020,
  title = {Iterative {{K-closest}} Point Algorithms for Colored Point Cloud Registration},
  author = {Choi, Ouk and Park, Min-Gyu and Hwang, Youngbae},
  date = {2020},
  journaltitle = {Sensors},
  volume = {20},
  number = {18},
  pages = {5331},
  publisher = {MDPI},
  doi = {10.3390/s20185331},
  url = {https://www.mdpi.com/1424-8220/20/18/5331},
  urldate = {2024-05-22},
  file = {C\:\\Users\\18317\\DevSpace\\DocHub\\essay\\zotero\\attachments\\GICP\\Choi et al_2020_Iterative K-closest point algorithms for colored point cloud registration.pdf;C\:\\Users\\18317\\Zotero\\storage\\G2EPG525\\5331.html}
}

@inproceedings{chungOrbeezslamRealtimeMonocular2023,
  title = {Orbeez-Slam: {{A}} Real-Time Monocular Visual Slam with Orb Features and Nerf-Realized Mapping},
  shorttitle = {Orbeez-Slam},
  booktitle = {2023 {{IEEE International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  author = {Chung, Chi-Ming and Tseng, Yang-Che and Hsu, Ya-Ching and Shi, Xiang-Qian and Hua, Yun-Hung and Yeh, Jia-Fong and Chen, Wen-Chin and Chen, Yi-Ting and Hsu, Winston H.},
  date = {2023},
  pages = {9400--9406},
  publisher = {IEEE},
  doi = {10.1109/ICRA48891.2023.10160950},
  url = {https://ieeexplore.ieee.org/abstract/document/10160950/},
  urldate = {2024-08-22},
  file = {C:\Users\18317\DevSpace\DocHub\essay\zotero\attachments\gs_spalt\introduce\Chung et al_2023_Orbeez-slam.pdf}
}

@article{czarnowskiDeepfactorsRealtimeProbabilistic2020,
  title = {Deepfactors: {{Real-time}} Probabilistic Dense Monocular Slam},
  shorttitle = {Deepfactors},
  author = {Czarnowski, Jan and Laidlow, Tristan and Clark, Ronald and Davison, Andrew J.},
  date = {2020},
  journaltitle = {IEEE Robotics and Automation Letters},
  volume = {5},
  number = {2},
  pages = {721--728},
  publisher = {IEEE},
  doi = {10.1109/LRA.2020.2965415},
  url = {https://ieeexplore.ieee.org/abstract/document/8954779/},
  urldate = {2024-09-03},
  file = {C:\Users\18317\DevSpace\DocHub\essay\zotero\attachments\gs_spalt\relate_works\Czarnowski et al_2020_Deepfactors.pdf}
}

@article{czarnowskiDeepfactorsRealtimeProbabilistic2020a,
  title = {Deepfactors: {{Real-time}} Probabilistic Dense Monocular Slam},
  shorttitle = {Deepfactors},
  author = {Czarnowski, Jan and Laidlow, Tristan and Clark, Ronald and Davison, Andrew J.},
  date = {2020},
  journaltitle = {IEEE Robotics and Automation Letters},
  volume = {5},
  number = {2},
  pages = {721--728},
  publisher = {IEEE},
  doi = {10.1109/LRA.2020.2965415},
  url = {https://ieeexplore.ieee.org/abstract/document/8954779/},
  urldate = {2024-09-03},
  file = {C:\Users\18317\DevSpace\DocHub\essay\zotero\attachments\gs_spalt\relate_works\Czarnowski et al_2020_Deepfactors2.pdf}
}

@article{daiBundleFusionRealTimeGlobally2017,
  title = {{{BundleFusion}}: {{Real-Time Globally Consistent 3D Reconstruction Using On-the-Fly Surface Reintegration}}},
  shorttitle = {{{BundleFusion}}},
  author = {Dai, Angela and Nießner, Matthias and Zollhöfer, Michael and Izadi, Shahram and Theobalt, Christian},
  date = {2017-08-31},
  journaltitle = {ACM Transactions on Graphics},
  shortjournal = {ACM Trans. Graph.},
  volume = {36},
  number = {4},
  pages = {1},
  issn = {0730-0301, 1557-7368},
  doi = {10.1145/3072959.3054739},
  url = {https://dl.acm.org/doi/10.1145/3072959.3054739},
  urldate = {2024-09-03},
  abstract = {Real-time, high-quality, 3D scanning of large-scale scenes is key to mixed reality and robotic applications. However, scalability brings challenges of drift in pose estimation, introducing significant errors in the accumulated model. Approaches often require hours of offline processing to globally correct model errors. Recent online methods demonstrate compelling results but suffer from (1) needing minutes to perform online correction, preventing true real-time use; (2) brittle frame-to-frame (or frame-to-model) pose estimation, resulting in many tracking failures; or (3) supporting only unstructured point-based representations, which limit scan quality and applicability. We systematically address these issues with a novel, real-time, end-to-end reconstruction framework. At its core is a robust pose estimation strategy, optimizing per frame for a global set of camera poses by considering the complete history of RGB-D input with an efficient hierarchical approach. We remove the heavy reliance on temporal tracking and continually localize to the globally optimized frames instead. We contribute a parallelizable optimization framework, which employs correspondences based on sparse features and dense geometric and photometric matching. Our approach estimates globally optimized (i.e., bundle adjusted) poses in real time, supports robust tracking with recovery from gross tracking failures (i.e., relocalization), and re-estimates the 3D model in real time to ensure global consistency, all within a single framework. Our approach outperforms state-of-the-art online systems with quality on par to offline methods, but with unprecedented speed and scan completeness. Our framework leads to a comprehensive online scanning solution for large indoor environments, enabling ease of use and high-quality results.               1},
  langid = {english},
  file = {C\:\\Users\\18317\\DevSpace\\DocHub\\essay\\zotero\\attachments\\gs_spalt\\relate_works\\Dai et al_2017_BundleFusion.pdf;C\:\\Users\\18317\\DevSpace\\DocHub\\essay\\zotero\\attachments\\gs_spalt\\relate_works\\Dai et al_2017_BundleFusion2.pdf}
}

@article{davisonMonoSLAMRealtimeSingle2007,
  title = {{{MonoSLAM}}: {{Real-time}} Single Camera {{SLAM}}},
  shorttitle = {{{MonoSLAM}}},
  author = {Davison, Andrew J. and Reid, Ian D. and Molton, Nicholas D. and Stasse, Olivier},
  date = {2007},
  journaltitle = {IEEE transactions on pattern analysis and machine intelligence},
  volume = {29},
  number = {6},
  pages = {1052--1067},
  publisher = {IEEE},
  doi = {10.1109/TPAMI.2007.1049},
  url = {https://ieeexplore.ieee.org/abstract/document/4160954/},
  urldate = {2024-09-26},
  file = {C:\Users\18317\DevSpace\DocHub\essay\zotero\attachments\gs_spalt\introduce\Davison et al_2007_MonoSLAM.pdf}
}

@article{durrant-whyteSimultaneousLocalizationMapping2006,
  title = {Simultaneous Localization and Mapping: Part {{I}}},
  shorttitle = {Simultaneous Localization and Mapping},
  author = {Durrant-Whyte, Hugh and Bailey, Tim},
  date = {2006},
  journaltitle = {IEEE robotics \& automation magazine},
  volume = {13},
  number = {2},
  pages = {99--110},
  publisher = {IEEE},
  doi = {10.1109/MRA.2006.1638022},
  url = {https://ieeexplore.ieee.org/abstract/document/1638022/},
  urldate = {2024-09-26},
  file = {C:\Users\18317\DevSpace\DocHub\essay\zotero\attachments\gs_spalt\introduce\Durrant-Whyte_Bailey_2006_Simultaneous localization and mapping.pdf}
}

@article{engelDirectSparseOdometry2017,
  title = {Direct Sparse Odometry},
  author = {Engel, Jakob and Koltun, Vladlen and Cremers, Daniel},
  date = {2017},
  journaltitle = {IEEE transactions on pattern analysis and machine intelligence},
  volume = {40},
  number = {3},
  pages = {611--625},
  publisher = {IEEE},
  doi = {10.1109/TPAMI.2017.2658577},
  url = {https://ieeexplore.ieee.org/abstract/document/7898369/},
  urldate = {2024-09-03},
  keywords = {nosource}
}

@article{engelDirectSparseOdometry2017a,
  title = {Direct Sparse Odometry},
  author = {Engel, Jakob and Koltun, Vladlen and Cremers, Daniel},
  date = {2017},
  journaltitle = {IEEE transactions on pattern analysis and machine intelligence},
  volume = {40},
  number = {3},
  pages = {611--625},
  publisher = {IEEE},
  doi = {10.1109/TPAMI.2017.2658577},
  url = {https://ieeexplore.ieee.org/abstract/document/7898369/},
  urldate = {2024-09-03},
  keywords = {nosource}
}

@article{engelDirectSparseOdometry2017b,
  title = {Direct Sparse Odometry},
  author = {Engel, Jakob and Koltun, Vladlen and Cremers, Daniel},
  date = {2017},
  journaltitle = {IEEE transactions on pattern analysis and machine intelligence},
  volume = {40},
  number = {3},
  pages = {611--625},
  publisher = {IEEE},
  doi = {10.1109/TPAMI.2017.2658577},
  url = {https://ieeexplore.ieee.org/abstract/document/7898369/},
  urldate = {2024-09-03},
  keywords = {nosource}
}

@article{engelDirectSparseOdometry2017c,
  title = {Direct Sparse Odometry},
  author = {Engel, Jakob and Koltun, Vladlen and Cremers, Daniel},
  date = {2017},
  journaltitle = {IEEE transactions on pattern analysis and machine intelligence},
  volume = {40},
  number = {3},
  pages = {611--625},
  publisher = {IEEE},
  doi = {10.1109/TPAMI.2017.2658577},
  url = {https://ieeexplore.ieee.org/abstract/document/7898369/},
  urldate = {2024-09-03},
  keywords = {nosource}
}

@article{engelDirectSparseOdometry2017d,
  title = {Direct Sparse Odometry},
  author = {Engel, Jakob and Koltun, Vladlen and Cremers, Daniel},
  date = {2017},
  journaltitle = {IEEE transactions on pattern analysis and machine intelligence},
  volume = {40},
  number = {3},
  pages = {611--625},
  publisher = {IEEE},
  doi = {10.1109/TPAMI.2017.2658577},
  url = {https://ieeexplore.ieee.org/abstract/document/7898369/},
  urldate = {2024-09-03},
  file = {C:\Users\18317\Zotero\storage\JW7M2VJI\Engel et al. - 2017 - Direct sparse odometry.pdf}
}

@article{engelDirectSparseOdometry2017e,
  title = {Direct Sparse Odometry},
  author = {Engel, Jakob and Koltun, Vladlen and Cremers, Daniel},
  date = {2017},
  journaltitle = {IEEE transactions on pattern analysis and machine intelligence},
  volume = {40},
  number = {3},
  pages = {611--625},
  publisher = {IEEE},
  doi = {10.1109/TPAMI.2017.2658577},
  url = {https://ieeexplore.ieee.org/abstract/document/7898369/},
  urldate = {2024-09-26}
}

@online{fanInstantSplatUnboundedSparseview2024,
  title = {{{InstantSplat}}: {{Unbounded Sparse-view Pose-free Gaussian Splatting}} in 40 {{Seconds}}},
  shorttitle = {{{InstantSplat}}},
  author = {Fan, Zhiwen and Cong, Wenyan and Wen, Kairun and Wang, Kevin and Zhang, Jian and Ding, Xinghao and Xu, Danfei and Ivanovic, Boris and Pavone, Marco and Pavlakos, Georgios and Wang, Zhangyang and Wang, Yue},
  date = {2024-06-30},
  eprint = {2403.20309},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2403.20309},
  url = {http://arxiv.org/abs/2403.20309},
  urldate = {2024-07-10},
  abstract = {While novel view synthesis (NVS) from a sparse set of images has advanced significantly in 3D computer vision, it relies on precise initial estimation of camera parameters using Structure-from-Motion (SfM). For instance, the recently developed Gaussian Splatting depends heavily on the accuracy of SfM-derived points and poses. However, SfM processes are time-consuming and often prove unreliable in sparse-view scenarios, where matched features are scarce, leading to accumulated errors and limited generalization capability across datasets. In this study, we introduce a novel and efficient framework to enhance robust NVS from sparse-view images. Our framework, InstantSplat, integrates multi-view stereo(MVS) predictions with point-based representations to construct 3D Gaussians of large-scale scenes from sparse-view data within seconds, addressing the aforementioned performance and efficiency issues by SfM. Specifically, InstantSplat generates densely populated surface points across all training views and determines the initial camera parameters using pixel-alignment. Nonetheless, the MVS points are not globally accurate, and the pixel-wise prediction from all views results in an excessive Gaussian number, yielding a overparameterized scene representation that compromises both training speed and accuracy. To address this issue, we employ a grid-based, confidence-aware Farthest Point Sampling to strategically position point primitives at representative locations in parallel. Next, we enhance pose accuracy and tune scene parameters through a gradient-based joint optimization framework from self-supervision. By employing this simplified framework, InstantSplat achieves a substantial reduction in training time, from hours to mere seconds, and demonstrates robust performance across various numbers of views in diverse datasets.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\18317\\DevSpace\\DocHub\\essay\\zotero\\attachments\\gs_spalt\\Fan et al_2024_InstantSplat.pdf;C\:\\Users\\18317\\Zotero\\storage\\HMLQE8XI\\2403.html}
}

@inproceedings{forsterSVOFastSemidirect2014,
  title = {{{SVO}}: {{Fast}} Semi-Direct Monocular Visual Odometry},
  shorttitle = {{{SVO}}},
  booktitle = {2014 {{IEEE}} International Conference on Robotics and Automation ({{ICRA}})},
  author = {Forster, Christian and Pizzoli, Matia and Scaramuzza, Davide},
  date = {2014},
  pages = {15--22},
  publisher = {IEEE},
  doi = {10.1109/ICRA.2014.6906584},
  url = {https://ieeexplore.ieee.org/abstract/document/6906584/},
  urldate = {2024-09-03},
  file = {C:\Users\18317\DevSpace\DocHub\essay\zotero\attachments\gs_spalt\relate_works\Forster et al_2014_SVO.pdf}
}

@inproceedings{fridovich-keilPlenoxelsRadianceFields2022,
  title = {Plenoxels: {{Radiance}} Fields without Neural Networks},
  shorttitle = {Plenoxels},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF}} Conference on Computer Vision and Pattern Recognition},
  author = {Fridovich-Keil, Sara and Yu, Alex and Tancik, Matthew and Chen, Qinhong and Recht, Benjamin and Kanazawa, Angjoo},
  date = {2022},
  pages = {5501--5510},
  url = {http://openaccess.thecvf.com/content/CVPR2022/html/Fridovich-Keil_Plenoxels_Radiance_Fields_Without_Neural_Networks_CVPR_2022_paper.html},
  urldate = {2024-09-03},
  keywords = {⛔ No DOI found},
  file = {C:\Users\18317\DevSpace\DocHub\essay\zotero\attachments\gs_spalt\relate_works\Fridovich-Keil et al_2022_Plenoxels.pdf}
}

@inproceedings{garbinFastnerfHighfidelityNeural2021,
  title = {Fastnerf: {{High-fidelity}} Neural Rendering at 200fps},
  shorttitle = {Fastnerf},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF}} International Conference on Computer Vision},
  author = {Garbin, Stephan J. and Kowalski, Marek and Johnson, Matthew and Shotton, Jamie and Valentin, Julien},
  date = {2021},
  pages = {14346--14355},
  url = {http://openaccess.thecvf.com/content/ICCV2021/html/Garbin_FastNeRF_High-Fidelity_Neural_Rendering_at_200FPS_ICCV_2021_paper.html},
  urldate = {2024-09-26},
  keywords = {⛔ No DOI found},
  file = {C:\Users\18317\DevSpace\DocHub\essay\zotero\attachments\gs_spalt\relate_works\Garbin et al_2021_Fastnerf.pdf}
}

@article{gauglitzEvaluationInterestPoint2011,
  title = {Evaluation of {{Interest Point Detectors}} and {{Feature Descriptors}} for {{Visual Tracking}}},
  author = {Gauglitz, Steffen and Höllerer, Tobias and Turk, Matthew},
  date = {2011-09},
  journaltitle = {International Journal of Computer Vision},
  shortjournal = {Int J Comput Vis},
  volume = {94},
  number = {3},
  pages = {335--360},
  issn = {0920-5691, 1573-1405},
  doi = {10.1007/s11263-011-0431-5},
  url = {http://link.springer.com/10.1007/s11263-011-0431-5},
  urldate = {2024-09-26},
  langid = {english},
  file = {C:\Users\18317\DevSpace\DocHub\essay\zotero\attachments\gs_spalt\relate_works\Gauglitz et al_2011_Evaluation of Interest Point Detectors and Feature Descriptors for Visual.pdf}
}

@article{guptaNDT6DColorRegistration2023,
  title = {{{NDT}}‐{{6D}} for Color Registration in Agri‐robotic Applications},
  author = {Gupta, Himanshu and Lilienthal, Achim J. and Andreasson, Henrik and Kurtser, Polina},
  date = {2023-09},
  journaltitle = {Journal of Field Robotics},
  shortjournal = {Journal of Field Robotics},
  volume = {40},
  number = {6},
  pages = {1603--1619},
  issn = {1556-4959, 1556-4967},
  doi = {10.1002/rob.22194},
  url = {https://onlinelibrary.wiley.com/doi/10.1002/rob.22194},
  urldate = {2024-05-20},
  abstract = {Abstract             Registration of point cloud data containing both depth and color information is critical for a variety of applications, including in‐field robotic plant manipulation, crop growth modeling, and autonomous navigation. However, current state‐of‐the‐art registration methods often fail in challenging agricultural field conditions due to factors such as occlusions, plant density, and variable illumination. To address these issues, we propose the NDT‐6D registration method, which is a color‐based variation of the Normal Distribution Transform (NDT) registration approach for point clouds. Our method computes correspondences between pointclouds using both geometric and color information and minimizes the distance between these correspondences using only the three‐dimensional (3D) geometric dimensions. We evaluate the method using the GRAPES3D data set collected with a commercial‐grade RGB‐D sensor mounted on a mobile platform in a vineyard. Results show that registration methods that only rely on depth information fail to provide quality registration for the tested data set. The proposed color‐based variation outperforms state‐of‐the‐art methods with a root mean square error (RMSE) of 1.1–1.6\,cm for NDT‐6D compared with 1.1–2.3\,cm for other color‐information‐based methods and 1.2–13.7\,cm for noncolor‐information‐based methods. The proposed method is shown to be robust against noises using the TUM RGBD data set by artificially adding noise present in an outdoor scenario. The relative pose error (RPE) increased 14\% for our method compared to an increase of 75\% for the best‐performing registration method. The obtained average accuracy suggests that the NDT‐6D registration methods can be used for in‐field precision agriculture applications, for example, crop detection, size‐based maturity estimation, and growth modeling.},
  langid = {english},
  keywords = {⭐},
  file = {C\:\\Users\\18317\\DevSpace\\DocHub\\essay\\zotero\\attachments\\GICP\\Gupta et al_2023_NDT‐6D for color registration in agri‐robotic applications2.pdf;C\:\\Users\\18317\\DevSpace\\DocHub\\essay\\zotero\\attachments\\GICP\\Gupta et al. - 2023 - NDT‐6D for color registration in agri‐robotic appl.pdf}
}

@article{hanPCKRFPointCloud2024,
  title = {{{PCKRF}}: {{Point Cloud Completion}} and {{Keypoint Refinement With Fusion Data}} for {{6D Pose Estimation}}},
  shorttitle = {{{PCKRF}}},
  author = {Han, Yiheng and Zhan, Irvin Haozhe and Zeng, Long and Wang, Yu-Ping and Yi, Ran and Yu, Minjing and Lin, Matthieu Gaetan and Sheng, Jenny and Liu, Yong-Jin},
  date = {2024},
  journaltitle = {IEEE Transactions on Visualization and Computer Graphics},
  publisher = {IEEE},
  doi = {10.1109/TVCG.2024.3390122},
  url = {https://ieeexplore.ieee.org/abstract/document/10504632/},
  urldate = {2024-06-03},
  keywords = {nosource}
}

@article{hanPointCloudRegistration2024,
  title = {A {{Point Cloud Registration Framework}} with {{Color Information Integration}}},
  author = {Han, Tianyu and Zhang, Ruijie and Kan, Jiangming and Dong, Ruifang and Zhao, Xixuan and Yao, Shun},
  date = {2024},
  journaltitle = {Remote Sensing},
  volume = {16},
  number = {5},
  pages = {743},
  publisher = {MDPI},
  doi = {10.3390/rs16050743},
  file = {C\:\\Users\\18317\\DevSpace\\DocHub\\essay\\zotero\\attachments\\raw\\Han et al_2024_A Point Cloud Registration Framework with Color Information Integration2.pdf;C\:\\Users\\18317\\Zotero\\storage\\WYGGTTKR\\743.html}
}

@article{haoRobustPointCloud2023,
  title = {Robust {{Point Cloud Registration Network}} for {{Complex Conditions}}},
  author = {Hao, Ruidong and Wei, Zhongwei and He, Xu and Zhu, Kaifeng and He, Jiawei and Wang, Jun and Li, Muyu and Zhang, Lei and Lv, Zhuang and Zhang, Xin},
  date = {2023},
  journaltitle = {Sensors},
  volume = {23},
  number = {24},
  pages = {9837},
  publisher = {MDPI},
  doi = {10.3390/s23249837},
  url = {https://www.mdpi.com/1424-8220/23/24/9837},
  urldate = {2024-06-05},
  file = {C\:\\Users\\18317\\DevSpace\\DocHub\\essay\\zotero\\attachments\\raw\\Hao et al_2023_Robust Point Cloud Registration Network for Complex Conditions.pdf;C\:\\Users\\18317\\Zotero\\storage\\AZ886ZKY\\Hao et al_2023_Robust Point Cloud Registration Network for Complex Conditions-dual-translated.pdf;C\:\\Users\\18317\\Zotero\\storage\\6EJLBKJD\\9837.html}
}

@online{haRGBDGSICPSLAM2024,
  title = {{{RGBD GS-ICP SLAM}}},
  author = {Ha, Seongbo and Yeon, Jiung and Yu, Hyeonwoo},
  date = {2024-03-22},
  eprint = {2403.12550},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2403.12550},
  urldate = {2024-05-23},
  abstract = {Simultaneous Localization and Mapping (SLAM) with dense representation plays a key role in robotics, Virtual Reality (VR), and Augmented Reality (AR) applications. Recent advancements in dense representation SLAM have highlighted the potential of leveraging neural scene representation and 3D Gaussian representation for high-fidelity spatial representation. In this paper, we propose a novel dense representation SLAM approach with a fusion of Generalized Iterative Closest Point (G-ICP) and 3D Gaussian Splatting (3DGS). In contrast to existing methods, we utilize a single Gaussian map for both tracking and mapping, resulting in mutual benefits. Through the exchange of covariances between tracking and mapping processes with scale alignment techniques, we minimize redundant computations and achieve an efficient system. Additionally, we enhance tracking accuracy and mapping quality through our keyframe selection methods. Experimental results demonstrate the effectiveness of our approach, showing an incredibly fast speed up to 107 FPS (for the entire system) and superior quality of the reconstructed map.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\18317\\Zotero\\storage\\HJ4D53M9\\Ha et al. - 2024 - RGBD GS-ICP SLAM.pdf;C\:\\Users\\18317\\Zotero\\storage\\P52BZ3I7\\2403.html}
}

@article{heIGICPIntensityGeometry2023,
  title = {{{IGICP}}: {{Intensity}} and {{Geometry Enhanced LiDAR Odometry}}},
  shorttitle = {{{IGICP}}},
  author = {He, Li and Li, Wen and Guan, Yisheng and Zhang, Hong},
  date = {2023},
  journaltitle = {IEEE Transactions on Intelligent Vehicles},
  publisher = {IEEE},
  doi = {10.1109/TIV.2023.3336376},
  url = {https://ieeexplore.ieee.org/abstract/document/10328702/},
  urldate = {2024-06-03},
  keywords = {⛔ No DOI found,nosource}
}

@article{hondaGeneralizedLOAMLiDAR2022,
  title = {Generalized {{LOAM}}: {{LiDAR Odometry Estimation With Trainable Local Geometric Features}}},
  shorttitle = {Generalized {{LOAM}}},
  author = {Honda, Kohei and Koide, Kenji and Yokozuka, Masashi and Oishi, Shuji and Banno, Atsuhiko},
  date = {2022},
  journaltitle = {IEEE Robotics and Automation Letters},
  volume = {7},
  number = {4},
  pages = {12459--12466},
  publisher = {IEEE},
  doi = {10.1109/LRA.2022.3219022},
  url = {https://ieeexplore.ieee.org/abstract/document/9935115/},
  urldate = {2024-05-22},
  file = {C:\Users\18317\DevSpace\DocHub\essay\zotero\attachments\raw\Honda et al_2022_Generalized LOAM.pdf}
}

@inproceedings{huang2DGaussianSplatting2024,
  title = {{{2D Gaussian Splatting}} for {{Geometrically Accurate Radiance Fields}}},
  booktitle = {Special {{Interest Group}} on {{Computer Graphics}} and {{Interactive Techniques Conference Conference Papers}} '24},
  author = {Huang, Binbin and Yu, Zehao and Chen, Anpei and Geiger, Andreas and Gao, Shenghua},
  date = {2024-07-13},
  eprint = {2403.17888},
  eprinttype = {arXiv},
  eprintclass = {cs},
  pages = {1--11},
  doi = {10.1145/3641519.3657428},
  url = {http://arxiv.org/abs/2403.17888},
  urldate = {2024-07-16},
  abstract = {3D Gaussian Splatting (3DGS) has recently revolutionized radiance field reconstruction, achieving high quality novel view synthesis and fast rendering speed without baking. However, 3DGS fails to accurately represent surfaces due to the multi-view inconsistent nature of 3D Gaussians. We present 2D Gaussian Splatting (2DGS), a novel approach to model and reconstruct geometrically accurate radiance fields from multi-view images. Our key idea is to collapse the 3D volume into a set of 2D oriented planar Gaussian disks. Unlike 3D Gaussians, 2D Gaussians provide view-consistent geometry while modeling surfaces intrinsically. To accurately recover thin surfaces and achieve stable optimization, we introduce a perspective-correct 2D splatting process utilizing ray-splat intersection and rasterization. Additionally, we incorporate depth distortion and normal consistency terms to further enhance the quality of the reconstructions. We demonstrate that our differentiable renderer allows for noise-free and detailed geometry reconstruction while maintaining competitive appearance quality, fast training speed, and real-time rendering.},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Graphics},
  file = {C\:\\Users\\18317\\DevSpace\\DocHub\\essay\\zotero\\attachments\\gs_spalt\\Huang et al_2024_2D Gaussian Splatting for Geometrically Accurate Radiance Fields.pdf;C\:\\Users\\18317\\Zotero\\storage\\WPHT7MYY\\2403.html}
}

@inproceedings{huangPhotoSLAMRealtimeSimultaneous2024,
  title = {Photo-{{SLAM}}: {{Real-time Simultaneous Localization}} and {{Photorealistic Mapping}} for {{Monocular Stereo}} and {{RGB-D Cameras}}},
  shorttitle = {Photo-{{SLAM}}},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Huang, Huajian and Li, Longwei and Cheng, Hui and Yeung, Sai-Kit},
  date = {2024},
  pages = {21584--21593},
  url = {https://openaccess.thecvf.com/content/CVPR2024/html/Huang_Photo-SLAM_Real-time_Simultaneous_Localization_and_Photorealistic_Mapping_for_Monocular_Stereo_CVPR_2024_paper.html},
  urldate = {2024-07-16},
  keywords = {⛔ No DOI found},
  file = {C:\Users\18317\DevSpace\DocHub\essay\zotero\attachments\gs_spalt\Huang et al_2024_Photo-SLAM2.pdf}
}

@online{huCGSLAMEfficientDense2024,
  title = {{{CG-SLAM}}: {{Efficient Dense RGB-D SLAM}} in a {{Consistent Uncertainty-aware 3D Gaussian Field}}},
  shorttitle = {{{CG-SLAM}}},
  author = {Hu, Jiarui and Chen, Xianhao and Feng, Boyin and Li, Guanglin and Yang, Liangjing and Bao, Hujun and Zhang, Guofeng and Cui, Zhaopeng},
  date = {2024-03-24},
  eprint = {2403.16095},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2403.16095},
  urldate = {2024-07-16},
  abstract = {Recently neural radiance fields (NeRF) have been widely exploited as 3D representations for dense simultaneous localization and mapping (SLAM). Despite their notable successes in surface modeling and novel view synthesis, existing NeRF-based methods are hindered by their computationally intensive and time-consuming volume rendering pipeline. This paper presents an efficient dense RGB-D SLAM system, i.e., CG-SLAM, based on a novel uncertainty-aware 3D Gaussian field with high consistency and geometric stability. Through an in-depth analysis of Gaussian Splatting, we propose several techniques to construct a consistent and stable 3D Gaussian field suitable for tracking and mapping. Additionally, a novel depth uncertainty model is proposed to ensure the selection of valuable Gaussian primitives during optimization, thereby improving tracking efficiency and accuracy. Experiments on various datasets demonstrate that CG-SLAM achieves superior tracking and mapping performance with a notable tracking speed of up to 15 Hz. We will make our source code publicly available. Project page: https://zju3dv.github.io/cg-slam.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Robotics},
  file = {C\:\\Users\\18317\\DevSpace\\DocHub\\essay\\zotero\\attachments\\gs_spalt\\Hu et al_2024_CG-SLAM3.pdf;C\:\\Users\\18317\\Zotero\\storage\\EKRNMV59\\2403.html}
}

@inproceedings{jatavallabhulaSlamDenseSlam2020,
  title = {∇ Slam: {{Dense}} Slam Meets Automatic Differentiation},
  shorttitle = {∇ Slam},
  booktitle = {2020 {{IEEE International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  author = {Jatavallabhula, Krishna Murthy and Iyer, Ganesh and Paull, Liam},
  date = {2020},
  pages = {2130--2137},
  publisher = {IEEE},
  doi = {10.1109/ICRA40945.2020.9197519},
  url = {https://ieeexplore.ieee.org/abstract/document/9197519/},
  urldate = {2024-08-06},
  keywords = {nosource}
}

@inproceedings{johariEslamEfficientDense2023,
  title = {Eslam: {{Efficient}} Dense Slam System Based on Hybrid Representation of Signed Distance Fields},
  shorttitle = {Eslam},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Johari, Mohammad Mahdi and Carta, Camilla and Fleuret, François},
  date = {2023},
  pages = {17408--17419},
  url = {http://openaccess.thecvf.com/content/CVPR2023/html/Johari_ESLAM_Efficient_Dense_SLAM_System_Based_on_Hybrid_Representation_of_CVPR_2023_paper.html},
  urldate = {2024-09-03},
  keywords = {⛔ No DOI found},
  file = {C:\Users\18317\DevSpace\DocHub\essay\zotero\attachments\gs_spalt\relate_works\Johari et al_2023_Eslam.pdf}
}

@article{johnsonRegistrationIntegrationTextured1999,
  title = {Registration and Integration of Textured {{3D}} Data},
  author = {Johnson, Andrew Edie and Kang, Sing Bing},
  date = {1999},
  journaltitle = {Image and vision computing},
  volume = {17},
  number = {2},
  pages = {135--147},
  publisher = {Elsevier},
  doi = {10.1016/S0262-8856(98)00117-6},
  url = {https://www.sciencedirect.com/science/article/pii/S0262885698001176},
  urldate = {2024-06-03},
  file = {C:\Users\18317\DevSpace\DocHub\essay\zotero\attachments\GICP\Johnson_Kang_1999_Registration and integration of textured 3D data.pdf}
}

@article{johnsonRegistrationIntegrationTextured1999a,
  title = {Registration and Integration of Textured {{3D}} Data},
  author = {Johnson, Andrew Edie and Kang, Sing Bing},
  date = {1999},
  journaltitle = {Image and vision computing},
  volume = {17},
  number = {2},
  pages = {135--147},
  publisher = {Elsevier},
  doi = {10.1016/S0262-8856(98)00117-6},
  url = {https://www.sciencedirect.com/science/article/pii/S0262885698001176},
  urldate = {2024-06-03},
  file = {C:\Users\18317\Zotero\storage\D2JTIT2J\Johnson_Kang_1999_Registration and integration of textured 3D data2-dual-translated.pdf}
}

@article{kangPointCloudRegistration2024,
  title = {Point {{Cloud Registration Method Based}} on {{Geometric Constraint}} and {{Transformation Evaluation}}},
  author = {Kang, Chuanli and Geng, Chongming and Lin, Zitao and Zhang, Sai and Zhang, Siyao and Wang, Shiwei},
  date = {2024},
  journaltitle = {Sensors},
  volume = {24},
  number = {6},
  pages = {1853},
  publisher = {MDPI},
  doi = {10.3390/s24061853},
  url = {https://www.mdpi.com/1424-8220/24/6/1853},
  urldate = {2024-06-05},
  file = {C\:\\Users\\18317\\Zotero\\storage\\YRVEY83S\\Kang et al_2024_Point Cloud Registration Method Based on Geometric Constraint and-dual-translated.pdf;C\:\\Users\\18317\\Zotero\\storage\\5887S4MM\\1853.html}
}

@article{kanopoulosDesignImageEdge1988,
  title = {Design of an Image Edge Detection Filter Using the {{Sobel}} Operator},
  author = {Kanopoulos, Nick and Vasanthavada, Nagesh and Baker, Robert L.},
  date = {1988},
  journaltitle = {IEEE Journal of solid-state circuits},
  volume = {23},
  number = {2},
  pages = {358--367},
  publisher = {IEEE},
  doi = {10.1109/4.996},
  url = {https://ieeexplore.ieee.org/abstract/document/996/},
  urldate = {2024-08-09},
  keywords = {nosource}
}

@online{keethaSplaTAMSplatTrack2024,
  title = {{{SplaTAM}}: {{Splat}}, {{Track}} \& {{Map 3D Gaussians}} for {{Dense RGB-D SLAM}}},
  shorttitle = {{{SplaTAM}}},
  author = {Keetha, Nikhil and Karhade, Jay and Jatavallabhula, Krishna Murthy and Yang, Gengshan and Scherer, Sebastian and Ramanan, Deva and Luiten, Jonathon},
  date = {2024-04-16},
  eprint = {2312.02126},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2312.02126},
  urldate = {2024-06-09},
  abstract = {Dense simultaneous localization and mapping (SLAM) is crucial for robotics and augmented reality applications. However, current methods are often hampered by the non-volumetric or implicit way they represent a scene. This work introduces SplaTAM, an approach that, for the first time, leverages explicit volumetric representations, i.e., 3D Gaussians, to enable high-fidelity reconstruction from a single unposed RGB-D camera, surpassing the capabilities of existing methods. SplaTAM employs a simple online tracking and mapping system tailored to the underlying Gaussian representation. It utilizes a silhouette mask to elegantly capture the presence of scene density. This combination enables several benefits over prior representations, including fast rendering and dense optimization, quickly determining if areas have been previously mapped, and structured map expansion by adding more Gaussians. Extensive experiments show that SplaTAM achieves up to 2x superior performance in camera pose estimation, map construction, and novel-view synthesis over existing methods, paving the way for more immersive high-fidelity SLAM applications.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Robotics},
  file = {C\:\\Users\\18317\\Zotero\\storage\\WUQKHF5Y\\Keetha et al. - 2024 - SplaTAM Splat, Track & Map 3D Gaussians for Dense.pdf;C\:\\Users\\18317\\Zotero\\storage\\5A6XMJDM\\2312.html}
}

@inproceedings{kellerRealtime3dReconstruction2013,
  title = {Real-Time 3d Reconstruction in Dynamic Scenes Using Point-Based Fusion},
  booktitle = {2013 {{International Conference}} on {{3D Vision-3DV}} 2013},
  author = {Keller, Maik and Lefloch, Damien and Lambers, Martin and Izadi, Shahram and Weyrich, Tim and Kolb, Andreas},
  date = {2013},
  pages = {1--8},
  publisher = {IEEE},
  doi = {10.1109/3DV.2013.9},
  url = {https://ieeexplore.ieee.org/abstract/document/6599048/},
  urldate = {2024-09-03},
  file = {C:\Users\18317\DevSpace\DocHub\essay\zotero\attachments\gs_spalt\relate_works\Keller et al_2013_Real-time 3d reconstruction in dynamic scenes using point-based fusion.pdf}
}

@inproceedings{kendallPosenetConvolutionalNetwork2015,
  title = {Posenet: {{A}} Convolutional Network for Real-Time 6-Dof Camera Relocalization},
  shorttitle = {Posenet},
  booktitle = {Proceedings of the {{IEEE}} International Conference on Computer Vision},
  author = {Kendall, Alex and Grimes, Matthew and Cipolla, Roberto},
  date = {2015},
  pages = {2938--2946},
  url = {http://openaccess.thecvf.com/content_iccv_2015/html/Kendall_PoseNet_A_Convolutional_ICCV_2015_paper.html},
  urldate = {2024-09-26},
  keywords = {⛔ No DOI found},
  file = {C:\Users\18317\DevSpace\DocHub\essay\zotero\attachments\gs_spalt\introduce\Kendall et al_2015_Posenet.pdf}
}

@article{kerbl3DGaussianSplatting2023,
  title = {{{3D Gaussian Splatting}} for {{Real-Time Radiance Field Rendering}}},
  author = {Kerbl, Bernhard and Kopanas, Georgios and Leimkühler, Thomas and Drettakis, George},
  date = {2023},
  journaltitle = {ACM Transactions on Graphics},
  volume = {42},
  number = {4},
  pages = {1--14},
  publisher = {ACM},
  doi = {10.1145/3592433},
  url = {https://sgvr.kaist.ac.kr/~sungeui/ICG/Students/[CS482]%203D%20Gaussian%20Splatting%20for%20Real-Time%20Radiance%20Field%20Rendering.pdf},
  urldate = {2024-06-09},
  file = {C\:\\Users\\18317\\DevSpace\\DocHub\\essay\\zotero\\attachments\\gs_spalt\\Kerbl et al_2023_3d gaussian splatting for real-time radiance field rendering.pdf;C\:\\Users\\18317\\Zotero\\storage\\PRZ6GF98\\3D Gaussian Splatting for Real-Time Radiance Field Rendering - 3d_gaussian_splatting_low.pdf}
}

@inproceedings{kerlDenseVisualSLAM2013,
  title = {Dense Visual {{SLAM}} for {{RGB-D}} Cameras},
  booktitle = {2013 {{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}}},
  author = {Kerl, Christian and Sturm, Jürgen and Cremers, Daniel},
  date = {2013},
  pages = {2100--2106},
  publisher = {IEEE},
  doi = {10.1109/IROS.2013.6696650},
  url = {https://ieeexplore.ieee.org/abstract/document/6696650/},
  urldate = {2024-08-22},
  file = {C:\Users\18317\DevSpace\DocHub\essay\zotero\attachments\gs_spalt\Kerl et al_2013_Dense visual SLAM for RGB-D cameras.pdf}
}

@inproceedings{kerlDenseVisualSLAM2013a,
  title = {Dense Visual {{SLAM}} for {{RGB-D}} Cameras},
  booktitle = {2013 {{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}}},
  author = {Kerl, Christian and Sturm, Jürgen and Cremers, Daniel},
  date = {2013},
  pages = {2100--2106},
  publisher = {IEEE},
  doi = {10.1109/IROS.2013.6696650},
  url = {https://ieeexplore.ieee.org/abstract/document/6696650/},
  urldate = {2024-08-22},
  file = {C:\Users\18317\DevSpace\DocHub\essay\zotero\attachments\gs_spalt\introduce\Kerl et al_2013_Dense visual SLAM for RGB-D cameras.pdf}
}

@inproceedings{kerlRobustOdometryEstimation2013,
  title = {Robust Odometry Estimation for {{RGB-D}} Cameras},
  booktitle = {2013 {{IEEE}} International Conference on Robotics and Automation},
  author = {Kerl, Christian and Sturm, Jürgen and Cremers, Daniel},
  date = {2013},
  pages = {3748--3754},
  publisher = {IEEE},
  doi = {10.1109/ICRA.2013.6631104},
  url = {https://ieeexplore.ieee.org/abstract/document/6631104/},
  urldate = {2024-09-13},
  file = {C:\Users\18317\DevSpace\DocHub\essay\zotero\attachments\gs_spalt\relate_works\Kerl et al_2013_Robust odometry estimation for RGB-D cameras.pdf}
}

@incollection{keselmanApproximateDifferentiableRendering2022,
  title = {Approximate {{Differentiable Rendering}} with {{Algebraic Surfaces}}},
  booktitle = {Computer {{Vision}} – {{ECCV}} 2022},
  author = {Keselman, Leonid and Hebert, Martial},
  editor = {Avidan, Shai and Brostow, Gabriel and Cissé, Moustapha and Farinella, Giovanni Maria and Hassner, Tal},
  date = {2022},
  volume = {13692},
  pages = {596--614},
  publisher = {Springer Nature Switzerland},
  location = {Cham},
  doi = {10.1007/978-3-031-19824-3_35},
  url = {https://link.springer.com/10.1007/978-3-031-19824-3_35},
  urldate = {2024-09-03},
  isbn = {978-3-031-19823-6 978-3-031-19824-3},
  langid = {english},
  file = {C:\Users\18317\DevSpace\DocHub\essay\zotero\attachments\gs_spalt\relate_works\Keselman_Hebert_2022_Approximate Differentiable Rendering with Algebraic Surfaces.pdf}
}

@unpublished{kingmaAdamMethodStochastic2014,
  title = {Adam: {{A}} Method for Stochastic Optimization},
  shorttitle = {Adam},
  author = {Kingma, Diederik P.},
  date = {2014},
  eprint = {1412.6980},
  eprinttype = {arXiv},
  keywords = {⛔ No DOI found}
}

@inproceedings{koideExactPointCloud2023,
  title = {Exact {{Point Cloud Downsampling}} for {{Fast}} and {{Accurate Global Trajectory Optimization}}},
  booktitle = {2023 {{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}} ({{IROS}})},
  author = {Koide, Kenji and Oishi, Shuji and Yokozuka, Masashi and Banno, Atsuhiko},
  date = {2023},
  pages = {9175--9182},
  publisher = {IEEE},
  doi = {10.1109/IROS55552.2023.10342403},
  url = {https://ieeexplore.ieee.org/abstract/document/10342403/},
  urldate = {2024-06-05},
  file = {C:\Users\18317\Zotero\storage\UN2P4NW4\Koide et al. - 2023 - Exact Point Cloud Downsampling for Fast and Accura.pdf}
}

@article{koideGloballyConsistent3D2021,
  title = {Globally Consistent {{3D LiDAR}} Mapping with {{GPU-accelerated GICP}} Matching Cost Factors},
  author = {Koide, Kenji and Yokozuka, Masashi and Oishi, Shuji and Banno, Atsuhiko},
  date = {2021},
  journaltitle = {IEEE Robotics and Automation Letters},
  volume = {6},
  number = {4},
  pages = {8591--8598},
  publisher = {IEEE},
  doi = {10.1109/LRA.2021.3113043},
  url = {https://ieeexplore.ieee.org/abstract/document/9540294/},
  urldate = {2024-05-22},
  file = {C\:\\Users\\18317\\DevSpace\\DocHub\\essay\\zotero\\attachments\\raw\\Koide et al_2021_Globally consistent 3D LiDAR mapping with GPU-accelerated GICP matching cost.pdf;C\:\\Users\\18317\\Zotero\\storage\\NMDSF4QZ\\2109.html}
}

@inproceedings{koideVoxelizedGicpFast2021,
  title = {Voxelized Gicp for Fast and Accurate 3d Point Cloud Registration},
  booktitle = {2021 {{IEEE International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  author = {Koide, Kenji and Yokozuka, Masashi and Oishi, Shuji and Banno, Atsuhiko},
  date = {2021},
  pages = {11054--11059},
  publisher = {IEEE},
  doi = {10.1109/ICRA48506.2021.9560835},
  url = {https://ieeexplore.ieee.org/abstract/document/9560835/},
  urldate = {2024-05-22},
  file = {C:\Users\18317\DevSpace\DocHub\essay\zotero\attachments\GICP\Koide et al_2021_Voxelized gicp for fast and accurate 3d point cloud registration2.pdf}
}

@online{koliosDPPEDensePose2024,
  title = {{{DPPE}}: {{Dense Pose Estimation}} in a {{Plenoxels Environment}} Using {{Gradient Approximation}}},
  shorttitle = {{{DPPE}}},
  author = {Kolios, Christopher and Bahoo, Yeganeh and Saeedi, Sajad},
  date = {2024-03-15},
  eprint = {2403.10773},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2403.10773},
  urldate = {2024-06-12},
  abstract = {We present DPPE, a dense pose estimation algorithm that functions over a Plenoxels environment. Recent advances in neural radiance field techniques have shown that it is a powerful tool for environment representation. More recent neural rendering algorithms have significantly improved both training duration and rendering speed. Plenoxels introduced a fully-differentiable radiance field technique that uses Plenoptic volume elements contained in voxels for rendering, offering reduced training times and better rendering accuracy, while also eliminating the neural net component. In this work, we introduce a 6-DoF monocular RGB-only pose estimation procedure for Plenoxels, which seeks to recover the ground truth camera pose after a perturbation. We employ a variation on classical template matching techniques, using stochastic gradient descent to optimize the pose by minimizing errors in re-rendering. In particular, we examine an approach that takes advantage of the rapid rendering speed of Plenoxels to numerically approximate part of the pose gradient, using a central differencing technique. We show that such methods are effective in pose estimation. Finally, we perform ablations over key components of the problem space, with a particular focus on image subsampling and Plenoxel grid resolution. Project website: https://sites.google.com/view/dppe},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Robotics},
  file = {C\:\\Users\\18317\\DevSpace\\DocHub\\essay\\zotero\\attachments\\gs_spalt\\Kolios et al_2024_DPPE2.pdf;C\:\\Users\\18317\\Zotero\\storage\\XFLYNZ4R\\2403.html}
}

@inproceedings{kornColorSupportedGeneralizedICP2014,
  title = {Color Supported Generalized-{{ICP}}},
  booktitle = {2014 {{International Conference}} on {{Computer Vision Theory}} and {{Applications}} ({{VISAPP}})},
  author = {Korn, Michael and Holzkothen, Martin and Pauli, Josef},
  date = {2014},
  volume = {3},
  pages = {592--599},
  publisher = {IEEE},
  url = {https://ieeexplore.ieee.org/abstract/document/7295135/},
  urldate = {2024-05-22},
  keywords = {💫},
  file = {C:\Users\18317\DevSpace\DocHub\essay\zotero\attachments\GICP\Korn et al_2014_Color supported generalized-ICP2.pdf}
}

@inproceedings{kornColorSupportedGeneralizedICP2014a,
  title = {Color Supported Generalized-{{ICP}}},
  booktitle = {2014 {{International Conference}} on {{Computer Vision Theory}} and {{Applications}} ({{VISAPP}})},
  author = {Korn, Michael and Holzkothen, Martin and Pauli, Josef},
  date = {2014},
  volume = {3},
  pages = {592--599},
  publisher = {IEEE},
  url = {https://ieeexplore.ieee.org/abstract/document/7295135/},
  urldate = {2024-05-20},
  keywords = {⛔ No DOI found},
  file = {C:\Users\18317\DevSpace\DocHub\essay\zotero\attachments\EMPTY_COLLECTION_NAMEKorn et al\Korn et al_2014_Color supported generalized-ICP.pdf}
}

@book{kuipersQuaternionsRotationSequences1999,
  title = {Quaternions and Rotation Sequences: A Primer with Applications to Orbits, Aerospace, and Virtual Reality},
  shorttitle = {Quaternions and Rotation Sequences},
  author = {Kuipers, Jack B.},
  date = {1999},
  publisher = {Princeton university press},
  url = {https://books.google.com/books?hl=en&lr=&id=_Og9DwAAQBAJ&oi=fnd&pg=PR21&dq=kuipers+Quaternions+Rotation+Sequences+1999&ots=t3I3ky6Zut&sig=o3xQEsvNHM-t2AIySNcaxBhSc_I},
  urldate = {2024-09-13},
  file = {C\:\\Users\\18317\\DevSpace\\DocHub\\essay\\zotero\\attachments\\gs_spalt\\relate_works\\Kuipers_1999_Quaternions and rotation sequences.pdf;C\:\\Users\\18317\\Zotero\\storage\\6MZ26W74\\Kuipers - 1999 - Quaternions and rotation sequences a primer with .pdf}
}

@book{kuipersQuaternionsRotationSequences1999a,
  title = {Quaternions and Rotation Sequences: A Primer with Applications to Orbits, Aerospace, and Virtual Reality},
  shorttitle = {Quaternions and Rotation Sequences},
  author = {Kuipers, Jack B.},
  date = {1999},
  publisher = {Princeton university press},
  url = {https://books.google.com/books?hl=en&lr=&id=_Og9DwAAQBAJ&oi=fnd&pg=PR21&dq=Quaternions+and+Rotation+Sequences.&ots=t3I4mt43wv&sig=cnwQdG6REEgGcqAHDiVntDvSanM},
  urldate = {2024-09-26},
  file = {C:\Users\18317\DevSpace\DocHub\essay\zotero\attachments\gs_spalt\relate_works\Kuipers_1999_Quaternions and rotation sequences2.pdf}
}

@inproceedings{lassnerPulsarEfficientSpherebased2021,
  title = {Pulsar: {{Efficient}} Sphere-Based Neural Rendering},
  shorttitle = {Pulsar},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Lassner, Christoph and Zollhofer, Michael},
  date = {2021},
  pages = {1440--1449},
  url = {http://openaccess.thecvf.com/content/CVPR2021/html/Lassner_Pulsar_Efficient_Sphere-Based_Neural_Rendering_CVPR_2021_paper.html},
  urldate = {2024-06-17},
  keywords = {nosource}
}

@inproceedings{lassnerPulsarEfficientSpherebased2021a,
  title = {Pulsar: {{Efficient}} Sphere-Based Neural Rendering},
  shorttitle = {Pulsar},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Lassner, Christoph and Zollhofer, Michael},
  date = {2021},
  pages = {1440--1449},
  url = {http://openaccess.thecvf.com/content/CVPR2021/html/Lassner_Pulsar_Efficient_Sphere-Based_Neural_Rendering_CVPR_2021_paper.html},
  urldate = {2024-06-17},
  keywords = {⛔ No DOI found},
  file = {C:\Users\18317\DevSpace\DocHub\essay\zotero\attachments\gs_spalt\Lassner_Zollhofer_2021_Pulsar.pdf}
}

@article{leeAccurateVisualSimultaneous2023,
  title = {Accurate {{Visual Simultaneous Localization}} and {{Mapping}} ({{SLAM}}) against {{Around View Monitor}} ({{AVM}}) {{Distortion Error Using Weighted Generalized Iterative Closest Point}} ({{GICP}})},
  author = {Lee, Yangwoo and Kim, Minsoo and Ahn, Joonwoo and Park, Jaeheung},
  date = {2023},
  journaltitle = {Sensors},
  volume = {23},
  number = {18},
  pages = {7947},
  publisher = {MDPI},
  doi = {10.3390/s23187947},
  url = {https://www.mdpi.com/1424-8220/23/18/7947},
  urldate = {2024-05-22},
  file = {C\:\\Users\\18317\\DevSpace\\DocHub\\essay\\zotero\\attachments\\raw\\Lee et al_2023_Accurate Visual Simultaneous Localization and Mapping (SLAM) against Around.pdf;C\:\\Users\\18317\\Zotero\\storage\\SA68L3LU\\7947.html}
}

@article{liuGeneticAlgorithmbasedOptimization2022,
  title = {Genetic Algorithm-Based Optimization for Color Point Cloud Registration},
  author = {Liu, Dongsheng and Hong, Deyan and Wang, Siting and Chen, Yahui},
  date = {2022},
  journaltitle = {Frontiers in Bioengineering and Biotechnology},
  volume = {10},
  pages = {923736},
  publisher = {Frontiers},
  keywords = {⛔ No DOI found},
  file = {C\:\\Users\\18317\\DevSpace\\DocHub\\essay\\zotero\\attachments\\raw\\Liu et al_2022_Genetic algorithm-based optimization for color point cloud registration.pdf;C\:\\Users\\18317\\Zotero\\storage\\78ZUJMAH\\Liu et al_2022_Genetic algorithm-based optimization for color point cloud registration-dual-translated.pdf;C\:\\Users\\18317\\Zotero\\storage\\LPVE7MZD\\full.html}
}

@article{liuHierarchicalOptimization3D2020,
  title = {Hierarchical Optimization of {{3D}} Point Cloud Registration},
  author = {Liu, Huikai and Zhang, Yue and Lei, Linjian and Xie, Hui and Li, Yan and Sun, Shengli},
  date = {2020},
  journaltitle = {Sensors},
  volume = {20},
  number = {23},
  pages = {6999},
  publisher = {MDPI},
  doi = {10.3390/s20236999},
  url = {https://www.mdpi.com/1424-8220/20/23/6999},
  urldate = {2024-06-05},
  file = {C\:\\Users\\18317\\DevSpace\\DocHub\\essay\\zotero\\attachments\\raw\\Liu et al_2020_Hierarchical optimization of 3D point cloud registration.pdf;C\:\\Users\\18317\\Zotero\\storage\\CWH53HRL\\Liu et al_2020_Hierarchical optimization of 3D point cloud registration2-dual-translated.pdf;C\:\\Users\\18317\\Zotero\\storage\\RQYP6JXU\\6999.html}
}

@article{liuHierarchicalOptimization3D2020a,
  title = {Hierarchical Optimization of {{3D}} Point Cloud Registration},
  author = {Liu, Huikai and Zhang, Yue and Lei, Linjian and Xie, Hui and Li, Yan and Sun, Shengli},
  date = {2020},
  journaltitle = {Sensors},
  volume = {20},
  number = {23},
  pages = {6999},
  publisher = {MDPI},
  doi = {10.3390/s20236999},
  file = {C\:\\Users\\18317\\DevSpace\\DocHub\\essay\\zotero\\attachments\\raw\\Liu et al_2020_Hierarchical optimization of 3D point cloud registration2.pdf;C\:\\Users\\18317\\Zotero\\storage\\J2XP4PD3\\6999.html}
}

@online{liuMVSGaussianFastGeneralizable2024,
  title = {{{MVSGaussian}}: {{Fast Generalizable Gaussian Splatting Reconstruction}} from {{Multi-View Stereo}}},
  shorttitle = {{{MVSGaussian}}},
  author = {Liu, Tianqi and Wang, Guangcong and Hu, Shoukang and Shen, Liao and Ye, Xinyi and Zang, Yuhang and Cao, Zhiguo and Li, Wei and Liu, Ziwei},
  date = {2024-07-15},
  eprint = {2405.12218},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2405.12218},
  url = {http://arxiv.org/abs/2405.12218},
  urldate = {2024-07-16},
  abstract = {We present MVSGaussian, a new generalizable 3D Gaussian representation approach derived from Multi-View Stereo (MVS) that can efficiently reconstruct unseen scenes. Specifically, 1) we leverage MVS to encode geometry-aware Gaussian representations and decode them into Gaussian parameters. 2) To further enhance performance, we propose a hybrid Gaussian rendering that integrates an efficient volume rendering design for novel view synthesis. 3) To support fast fine-tuning for specific scenes, we introduce a multi-view geometric consistent aggregation strategy to effectively aggregate the point clouds generated by the generalizable model, serving as the initialization for per-scene optimization. Compared with previous generalizable NeRF-based methods, which typically require minutes of fine-tuning and seconds of rendering per image, MVSGaussian achieves real-time rendering with better synthesis quality for each scene. Compared with the vanilla 3D-GS, MVSGaussian achieves better view synthesis with less training computational cost. Extensive experiments on DTU, Real Forward-facing, NeRF Synthetic, and Tanks and Temples datasets validate that MVSGaussian attains state-of-the-art performance with convincing generalizability, real-time rendering speed, and fast per-scene optimization.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\18317\\DevSpace\\DocHub\\essay\\zotero\\attachments\\gs_spalt\\Liu et al_2024_MVSGaussian2.pdf;C\:\\Users\\18317\\Zotero\\storage\\37XWHTCP\\2405.html}
}

@article{liuNeuralSparseVoxel2020,
  title = {Neural Sparse Voxel Fields},
  author = {Liu, Lingjie and Gu, Jiatao and Zaw Lin, Kyaw and Chua, Tat-Seng and Theobalt, Christian},
  date = {2020},
  journaltitle = {Advances in Neural Information Processing Systems},
  volume = {33},
  pages = {15651--15663},
  url = {https://proceedings.neurips.cc/paper/2020/hash/b4b758962f17808746e9bb832a6fa4b8-Abstract.html},
  urldate = {2024-09-03},
  keywords = {⛔ No DOI found},
  file = {C:\Users\18317\DevSpace\DocHub\essay\zotero\attachments\gs_spalt\relate_works\Liu et al_2020_Neural sparse voxel fields.pdf}
}

@incollection{loperOpenDRApproximateDifferentiable2014,
  title = {{{OpenDR}}: {{An Approximate Differentiable Renderer}}},
  shorttitle = {{{OpenDR}}},
  booktitle = {Computer {{Vision}} – {{ECCV}} 2014},
  author = {Loper, Matthew M. and Black, Michael J.},
  editor = {Fleet, David and Pajdla, Tomas and Schiele, Bernt and Tuytelaars, Tinne},
  date = {2014},
  volume = {8695},
  pages = {154--169},
  publisher = {Springer International Publishing},
  location = {Cham},
  doi = {10.1007/978-3-319-10584-0_11},
  url = {http://link.springer.com/10.1007/978-3-319-10584-0_11},
  urldate = {2024-09-26},
  isbn = {978-3-319-10583-3 978-3-319-10584-0},
  langid = {english},
  file = {C:\Users\18317\DevSpace\DocHub\essay\zotero\attachments\gs_spalt\relate_works\Loper_Black_2014_OpenDR.pdf}
}

@online{luitenDynamic3DGaussians2023,
  title = {Dynamic {{3D Gaussians}}: {{Tracking}} by {{Persistent Dynamic View Synthesis}}},
  shorttitle = {Dynamic {{3D Gaussians}}},
  author = {Luiten, Jonathon and Kopanas, Georgios and Leibe, Bastian and Ramanan, Deva},
  date = {2023-08-18},
  eprint = {2308.09713},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2308.09713},
  urldate = {2024-09-03},
  abstract = {We present a method that simultaneously addresses the tasks of dynamic scene novel-view synthesis and six degree-of-freedom (6-DOF) tracking of all dense scene elements. We follow an analysis-by-synthesis framework, inspired by recent work that models scenes as a collection of 3D Gaussians which are optimized to reconstruct input images via differentiable rendering. To model dynamic scenes, we allow Gaussians to move and rotate over time while enforcing that they have persistent color, opacity, and size. By regularizing Gaussians' motion and rotation with local-rigidity constraints, we show that our Dynamic 3D Gaussians correctly model the same area of physical space over time, including the rotation of that space. Dense 6-DOF tracking and dynamic reconstruction emerges naturally from persistent dynamic view synthesis, without requiring any correspondence or flow as input. We demonstrate a large number of downstream applications enabled by our representation, including first-person view synthesis, dynamic compositional scene synthesis, and 4D video editing.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\18317\\DevSpace\\DocHub\\essay\\zotero\\attachments\\gs_spalt\\relate_works\\Luiten et al_2023_Dynamic 3D Gaussians.pdf;C\:\\Users\\18317\\Zotero\\storage\\JDXH6FQN\\2308.html}
}

@article{lyuDynamicDownsamplingAlgorithm2024,
  title = {Dynamic {{Downsampling Algorithm}} for {{3D Point Cloud Map Based}} on {{Voxel Filtering}}},
  author = {Lyu, Wenqi and Ke, Wei and Sheng, Hao and Ma, Xiao and Zhang, Huayun},
  date = {2024},
  journaltitle = {Applied Sciences},
  volume = {14},
  number = {8},
  pages = {3160},
  publisher = {MDPI},
  doi = {10.3390/app14083160},
  url = {https://www.mdpi.com/2076-3417/14/8/3160},
  urldate = {2024-06-05},
  file = {C\:\\Users\\18317\\DevSpace\\DocHub\\essay\\zotero\\attachments\\raw\\Lyu et al_2024_Dynamic Downsampling Algorithm for 3D Point Cloud Map Based on Voxel Filtering2.pdf;C\:\\Users\\18317\\Zotero\\storage\\EI2KE83E\\3160.html}
}

@article{mackiewiczPrincipalComponentsAnalysis1993,
  title = {Principal Components Analysis ({{PCA}})},
  author = {Maćkiewicz, Andrzej and Ratajczak, Waldemar},
  date = {1993},
  journaltitle = {Computers \& Geosciences},
  volume = {19},
  number = {3},
  pages = {303--342},
  publisher = {Elsevier},
  doi = {10.1016/0098-3004(93)90090-R},
  url = {https://www.sciencedirect.com/science/article/pii/009830049390090R},
  urldate = {2024-07-23},
  keywords = {nosource}
}

@inproceedings{matsukiGaussianSplattingSlam2024,
  title = {Gaussian Splatting Slam},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Matsuki, Hidenobu and Murai, Riku and Kelly, Paul HJ and Davison, Andrew J.},
  date = {2024},
  pages = {18039--18048},
  url = {https://openaccess.thecvf.com/content/CVPR2024/html/Matsuki_Gaussian_Splatting_SLAM_CVPR_2024_paper.html},
  urldate = {2024-07-17},
  keywords = {⛔ No DOI found},
  file = {C:\Users\18317\DevSpace\DocHub\essay\zotero\attachments\gs_spalt\Matsuki et al_2024_Gaussian splatting slam2.pdf}
}

@article{maxOpticalModelsDirect1995,
  title = {Optical Models for Direct Volume Rendering},
  author = {Max, Nelson},
  date = {1995},
  journaltitle = {IEEE Transactions on Visualization and Computer Graphics},
  volume = {1},
  number = {2},
  pages = {99--108},
  publisher = {IEEE},
  doi = {10.1109/2945.468400},
  url = {https://ieeexplore.ieee.org/abstract/document/468400/},
  urldate = {2024-09-26},
  file = {C:\Users\18317\DevSpace\DocHub\essay\zotero\attachments\gs_spalt\relate_works\Max_1995_Optical models for direct volume rendering.pdf}
}

@inproceedings{mccormacSemanticfusionDense3d2017,
  title = {Semanticfusion: {{Dense}} 3d Semantic Mapping with Convolutional Neural Networks},
  shorttitle = {Semanticfusion},
  booktitle = {2017 {{IEEE International Conference}} on {{Robotics}} and Automation ({{ICRA}})},
  author = {McCormac, John and Handa, Ankur and Davison, Andrew and Leutenegger, Stefan},
  date = {2017},
  pages = {4628--4635},
  publisher = {IEEE},
  doi = {10.1109/ICRA.2017.7989538},
  url = {https://ieeexplore.ieee.org/abstract/document/7989538/},
  urldate = {2024-09-03},
  file = {C:\Users\18317\DevSpace\DocHub\essay\zotero\attachments\gs_spalt\relate_works\McCormac et al_2017_Semanticfusion.pdf}
}

@inproceedings{menColorPointCloud2011,
  title = {Color Point Cloud Registration with {{4D ICP}} Algorithm},
  booktitle = {2011 {{IEEE International Conference}} on {{Robotics}} and {{Automation}}},
  author = {Men, Hao and Gebre, Biruk and Pochiraju, Kishore},
  date = {2011},
  pages = {1511--1516},
  publisher = {IEEE},
  doi = {10.1109/ICRA.2011.5980407},
  url = {https://ieeexplore.ieee.org/abstract/document/5980407/},
  urldate = {2024-05-25},
  file = {C\:\\Users\\18317\\DevSpace\\DocHub\\essay\\zotero\\attachments\\GICP\\Men et al_2011_Color point cloud registration with 4D ICP algorithm.pdf;C\:\\Users\\18317\\DevSpace\\DocHub\\essay\\zotero\\attachments\\GICP\\Men et al_2011_Color point cloud registration with 4D ICP algorithm2.pdf}
}

@article{mildenhallNeRFRepresentingScenes2022,
  title = {{{NeRF}}: Representing Scenes as Neural Radiance Fields for View Synthesis},
  shorttitle = {{{NeRF}}},
  author = {Mildenhall, Ben and Srinivasan, Pratul P. and Tancik, Matthew and Barron, Jonathan T. and Ramamoorthi, Ravi and Ng, Ren},
  date = {2022-01},
  journaltitle = {Communications of the ACM},
  shortjournal = {Commun. ACM},
  volume = {65},
  number = {1},
  pages = {99--106},
  issn = {0001-0782, 1557-7317},
  doi = {10.1145/3503250},
  url = {https://dl.acm.org/doi/10.1145/3503250},
  urldate = {2024-08-22},
  abstract = {We present a method that achieves state-of-the-art results for synthesizing novel views of complex scenes by optimizing an underlying continuous volumetric scene function using a sparse set of input views. Our algorithm represents a scene using a fully connected (nonconvolutional) deep network, whose input is a single continuous 5D coordinate (spatial location (               x               ,               y               ,               z               ) and viewing direction (               θ, ϕ               )) and whose output is the volume density and view-dependent emitted radiance at that spatial location. We synthesize views by querying 5D coordinates along camera rays and use classic volume rendering techniques to project the output colors and densities into an image. Because volume rendering is naturally differentiable, the only input required to optimize our representation is a set of images with known camera poses. We describe how to effectively optimize neural radiance fields to render photorealistic novel views of scenes with complicated geometry and appearance, and demonstrate results that outperform prior work on neural rendering and view synthesis.},
  langid = {english},
  file = {C:\Users\18317\DevSpace\DocHub\essay\zotero\attachments\gs_spalt\introduce\Mildenhall et al_2022_NeRF.pdf}
}

@article{mildenhallNeRFRepresentingScenes2022a,
  title = {{{NeRF}}: Representing Scenes as Neural Radiance Fields for View Synthesis},
  shorttitle = {{{NeRF}}},
  author = {Mildenhall, Ben and Srinivasan, Pratul P. and Tancik, Matthew and Barron, Jonathan T. and Ramamoorthi, Ravi and Ng, Ren},
  date = {2022-01},
  journaltitle = {Communications of the ACM},
  shortjournal = {Commun. ACM},
  volume = {65},
  number = {1},
  pages = {99--106},
  issn = {0001-0782, 1557-7317},
  doi = {10.1145/3503250},
  url = {https://dl.acm.org/doi/10.1145/3503250},
  urldate = {2024-08-22},
  abstract = {We present a method that achieves state-of-the-art results for synthesizing novel views of complex scenes by optimizing an underlying continuous volumetric scene function using a sparse set of input views. Our algorithm represents a scene using a fully connected (nonconvolutional) deep network, whose input is a single continuous 5D coordinate (spatial location (               x               ,               y               ,               z               ) and viewing direction (               θ, ϕ               )) and whose output is the volume density and view-dependent emitted radiance at that spatial location. We synthesize views by querying 5D coordinates along camera rays and use classic volume rendering techniques to project the output colors and densities into an image. Because volume rendering is naturally differentiable, the only input required to optimize our representation is a set of images with known camera poses. We describe how to effectively optimize neural radiance fields to render photorealistic novel views of scenes with complicated geometry and appearance, and demonstrate results that outperform prior work on neural rendering and view synthesis.},
  langid = {english},
  file = {C:\Users\18317\DevSpace\DocHub\essay\zotero\attachments\gs_spalt\introduce\Mildenhall et al_2022_NeRF2.pdf}
}

@article{mullerInstantNeuralGraphics2022,
  title = {Instant Neural Graphics Primitives with a Multiresolution Hash Encoding},
  author = {Müller, Thomas and Evans, Alex and Schied, Christoph and Keller, Alexander},
  date = {2022-07},
  journaltitle = {ACM Transactions on Graphics},
  shortjournal = {ACM Trans. Graph.},
  volume = {41},
  number = {4},
  pages = {1--15},
  issn = {0730-0301, 1557-7368},
  doi = {10.1145/3528223.3530127},
  url = {https://dl.acm.org/doi/10.1145/3528223.3530127},
  urldate = {2024-09-03},
  abstract = {Neural graphics primitives, parameterized by fully connected neural networks, can be costly to train and evaluate. We reduce this cost with a versatile new input encoding that permits the use of a smaller network without sacrificing quality, thus significantly reducing the number of floating point and memory access operations: a small neural network is augmented by a multiresolution hash table of trainable feature vectors whose values are optimized through stochastic gradient descent. The multiresolution structure allows the network to disambiguate hash collisions, making for a simple architecture that is trivial to parallelize on modern GPUs. We leverage this parallelism by implementing the whole system using fully-fused CUDA kernels with a focus on minimizing wasted bandwidth and compute operations. We achieve a combined speedup of several orders of magnitude, enabling training of high-quality neural graphics primitives in a matter of seconds, and rendering in tens of milliseconds at a resolution of 1920×1080.},
  langid = {english},
  file = {C:\Users\18317\DevSpace\DocHub\essay\zotero\attachments\gs_spalt\relate_works\Müller et al_2022_Instant neural graphics primitives with a multiresolution hash encoding.pdf}
}

@article{mur-artalOrbslam2OpensourceSlam2017,
  title = {Orb-Slam2: {{An}} Open-Source Slam System for Monocular, Stereo, and Rgb-d Cameras},
  shorttitle = {Orb-Slam2},
  author = {Mur-Artal, Raul and Tardós, Juan D.},
  date = {2017},
  journaltitle = {IEEE transactions on robotics},
  volume = {33},
  number = {5},
  pages = {1255--1262},
  publisher = {IEEE},
  doi = {10.1109/TRO.2017.2705103},
  url = {https://ieeexplore.ieee.org/abstract/document/7946260/},
  urldate = {2024-08-22},
  file = {C:\Users\18317\DevSpace\DocHub\essay\zotero\attachments\gs_spalt\experiments\Mur-Artal_Tardós_2017_Orb-slam2.pdf}
}

@article{namMapbasedMobileRobot2017,
  title = {A 2.5 {{D}} Map-Based Mobile Robot Localization via Cooperation of Aerial and Ground Robots},
  author = {Nam, Tae Hyeon and Shim, Jae Hong and Cho, Young Im},
  date = {2017},
  journaltitle = {Sensors},
  volume = {17},
  number = {12},
  pages = {2730},
  publisher = {MDPI},
  doi = {10.3390/s17122730},
  file = {C\:\\Users\\18317\\DevSpace\\DocHub\\essay\\zotero\\attachments\\raw\\Nam et al_2017_A 2.pdf;C\:\\Users\\18317\\Zotero\\storage\\97SG28UX\\Nam et al_2017_A 2-dual-translated.pdf;C\:\\Users\\18317\\Zotero\\storage\\DLTQ5SGA\\2730.html}
}

@inproceedings{newcombeDTAMDenseTracking2011,
  title = {{{DTAM}}: {{Dense}} Tracking and Mapping in Real-Time},
  shorttitle = {{{DTAM}}},
  booktitle = {2011 International Conference on Computer Vision},
  author = {Newcombe, Richard A. and Lovegrove, Steven J. and Davison, Andrew J.},
  date = {2011},
  pages = {2320--2327},
  publisher = {IEEE},
  doi = {10.1109/ICCV.2011.6126513},
  url = {https://ieeexplore.ieee.org/abstract/document/6126513/},
  urldate = {2024-09-13},
  file = {C:\Users\18317\DevSpace\DocHub\essay\zotero\attachments\gs_spalt\relate_works\Newcombe et al_2011_DTAM.pdf}
}

@inproceedings{newcombeKinectfusionRealtimeDense2011,
  title = {Kinectfusion: {{Real-time}} Dense Surface Mapping and Tracking},
  shorttitle = {Kinectfusion},
  booktitle = {2011 10th {{IEEE}} International Symposium on Mixed and Augmented Reality},
  author = {Newcombe, Richard A. and Izadi, Shahram and Hilliges, Otmar and Molyneaux, David and Kim, David and Davison, Andrew J. and Kohi, Pushmeet and Shotton, Jamie and Hodges, Steve and Fitzgibbon, Andrew},
  date = {2011},
  pages = {127--136},
  publisher = {Ieee},
  doi = {10.1109/ISMAR.2011.6092378},
  url = {https://ieeexplore.ieee.org/abstract/document/6162880/},
  urldate = {2024-09-03},
  file = {C:\Users\18317\DevSpace\DocHub\essay\zotero\attachments\gs_spalt\relate_works\Newcombe et al_2011_Kinectfusion.pdf}
}

@inproceedings{newcombeKinectfusionRealtimeDense2011a,
  title = {Kinectfusion: {{Real-time}} Dense Surface Mapping and Tracking},
  shorttitle = {Kinectfusion},
  booktitle = {2011 10th {{IEEE}} International Symposium on Mixed and Augmented Reality},
  author = {Newcombe, Richard A. and Izadi, Shahram and Hilliges, Otmar and Molyneaux, David and Kim, David and Davison, Andrew J. and Kohi, Pushmeet and Shotton, Jamie and Hodges, Steve and Fitzgibbon, Andrew},
  date = {2011},
  pages = {127--136},
  publisher = {Ieee},
  doi = {10.1109/ISMAR.2011.6092378},
  url = {https://ieeexplore.ieee.org/abstract/document/6162880/},
  urldate = {2024-09-26},
  file = {C:\Users\18317\DevSpace\DocHub\essay\zotero\attachments\gs_spalt\introduce\Newcombe et al_2011_Kinectfusion.pdf}
}

@inproceedings{niemeyerDifferentiableVolumetricRendering2020,
  title = {Differentiable Volumetric Rendering: {{Learning}} Implicit 3d Representations without 3d Supervision},
  shorttitle = {Differentiable Volumetric Rendering},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF}} Conference on Computer Vision and Pattern Recognition},
  author = {Niemeyer, Michael and Mescheder, Lars and Oechsle, Michael and Geiger, Andreas},
  date = {2020},
  pages = {3504--3515},
  url = {http://openaccess.thecvf.com/content_CVPR_2020/html/Niemeyer_Differentiable_Volumetric_Rendering_Learning_Implicit_3D_Representations_Without_3D_Supervision_CVPR_2020_paper.html},
  urldate = {2024-09-03},
  keywords = {⛔ No DOI found},
  file = {C:\Users\18317\DevSpace\DocHub\essay\zotero\attachments\gs_spalt\relate_works\Niemeyer et al_2020_Differentiable volumetric rendering.pdf}
}

@inproceedings{parkColoredPointCloud2017,
  title = {Colored Point Cloud Registration Revisited},
  booktitle = {Proceedings of the {{IEEE}} International Conference on Computer Vision},
  author = {Park, Jaesik and Zhou, Qian-Yi and Koltun, Vladlen},
  date = {2017},
  pages = {143--152},
  url = {http://openaccess.thecvf.com/content_iccv_2017/html/Park_Colored_Point_Cloud_ICCV_2017_paper.html},
  urldate = {2024-05-23},
  keywords = {💫},
  file = {C:\Users\18317\Zotero\storage\NIWZJR2Y\Park et al_2017_Colored point cloud registration revisited-dual-translated.pdf}
}

@online{pengRTGSLAMRealtime3D2024,
  title = {{{RTG-SLAM}}: {{Real-time 3D Reconstruction}} at {{Scale}} Using {{Gaussian Splatting}}},
  shorttitle = {{{RTG-SLAM}}},
  author = {Peng, Zhexi and Shao, Tianjia and Liu, Yong and Zhou, Jingke and Yang, Yin and Wang, Jingdong and Zhou, Kun},
  date = {2024-05-08},
  eprint = {2404.19706},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.1145/3658233},
  url = {http://arxiv.org/abs/2404.19706},
  urldate = {2024-07-16},
  abstract = {We present Real-time Gaussian SLAM (RTG-SLAM), a real-time 3D reconstruction system with an RGBD camera for large-scale environments using Gaussian splatting. The system features a compact Gaussian representation and a highly efficient on-the-fly Gaussian optimization scheme. We force each Gaussian to be either opaque or nearly transparent, with the opaque ones fitting the surface and dominant colors, and transparent ones fitting residual colors. By rendering depth in a different way from color rendering, we let a single opaque Gaussian well fit a local surface region without the need of multiple overlapping Gaussians, hence largely reducing the memory and computation cost. For on-the-fly Gaussian optimization, we explicitly add Gaussians for three types of pixels per frame: newly observed, with large color errors, and with large depth errors. We also categorize all Gaussians into stable and unstable ones, where the stable Gaussians are expected to well fit previously observed RGBD images and otherwise unstable. We only optimize the unstable Gaussians and only render the pixels occupied by unstable Gaussians. In this way, both the number of Gaussians to be optimized and pixels to be rendered are largely reduced, and the optimization can be done in real time. We show real-time reconstructions of a variety of large scenes. Compared with the state-of-the-art NeRF-based RGBD SLAM, our system achieves comparable high-quality reconstruction but with around twice the speed and half the memory cost, and shows superior performance in the realism of novel view synthesis and camera tracking accuracy.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\18317\\DevSpace\\DocHub\\essay\\zotero\\attachments\\gs_spalt\\Peng et al_2024_RTG-SLAM2.pdf;C\:\\Users\\18317\\Zotero\\storage\\XV5IHMT8\\2404.html}
}

@article{pomerleauComparingICPVariants2013,
  title = {Comparing {{ICP}} Variants on Real-World Data Sets: {{Open-source}} Library and Experimental Protocol},
  shorttitle = {Comparing {{ICP}} Variants on Real-World Data Sets},
  author = {Pomerleau, François and Colas, Francis and Siegwart, Roland and Magnenat, Stéphane},
  date = {2013-04},
  journaltitle = {Autonomous Robots},
  shortjournal = {Auton Robot},
  volume = {34},
  number = {3},
  pages = {133--148},
  issn = {0929-5593, 1573-7527},
  doi = {10.1007/s10514-013-9327-2},
  url = {http://link.springer.com/10.1007/s10514-013-9327-2},
  urldate = {2024-09-26},
  langid = {english},
  file = {C\:\\Users\\18317\\DevSpace\\DocHub\\essay\\zotero\\attachments\\gs_spalt\\relate_works\\Pomerleau et al_2013_Comparing ICP variants on real-world data sets.pdf;C\:\\Users\\18317\\DevSpace\\DocHub\\essay\\zotero\\attachments\\gs_spalt\\relate_works\\Pomerleau et al_2013_Comparing ICP variants on real-world data sets2.pdf}
}

@online{prisacariuFrameworkVolumetricIntegration2014,
  title = {A {{Framework}} for the {{Volumetric Integration}} of {{Depth Images}}},
  author = {Prisacariu, Victor Adrian and Kähler, Olaf and Cheng, Ming Ming and Ren, Carl Yuheng and Valentin, Julien and Torr, Philip H. S. and Reid, Ian D. and Murray, David W.},
  date = {2014-10-23},
  eprint = {1410.0925},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1410.0925},
  urldate = {2024-09-03},
  abstract = {Volumetric models have become a popular representation for 3D scenes in recent years. One of the breakthroughs leading to their popularity was KinectFusion, where the focus is on 3D reconstruction using RGB-D sensors. However, monocular SLAM has since also been tackled with very similar approaches. Representing the reconstruction volumetrically as a truncated signed distance function leads to most of the simplicity and efficiency that can be achieved with GPU implementations of these systems. However, this representation is also memory-intensive and limits the applicability to small scale reconstructions. Several avenues have been explored for overcoming this limitation. With the aim of summarizing them and providing for a fast and flexible 3D reconstruction pipeline, we propose a new, unifying framework called InfiniTAM. The core idea is that individual steps like camera tracking, scene representation and integration of new data can easily be replaced and adapted to the needs of the user. Along with the framework we also provide a set of components for scalable reconstruction: two implementations of camera trackers, based on RGB data and on depth data, two representations of the 3D volumetric data, a dense volume and one based on hashes of subblocks, and an optional module for swapping subblocks in and out of the typically limited GPU memory.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\18317\\DevSpace\\DocHub\\essay\\zotero\\attachments\\gs_spalt\\relate_works\\Prisacariu et al_2014_A Framework for the Volumetric Integration of Depth Images.pdf;C\:\\Users\\18317\\Zotero\\storage\\EYDXAEL2\\1410.html}
}

@online{raviAccelerating3DDeep2020,
  title = {Accelerating {{3D Deep Learning}} with {{PyTorch3D}}},
  author = {Ravi, Nikhila and Reizenstein, Jeremy and Novotny, David and Gordon, Taylor and Lo, Wan-Yen and Johnson, Justin and Gkioxari, Georgia},
  date = {2020-07-16},
  eprint = {2007.08501},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2007.08501},
  urldate = {2024-06-17},
  abstract = {Deep learning has significantly improved 2D image recognition. Extending into 3D may advance many new applications including autonomous vehicles, virtual and augmented reality, authoring 3D content, and even improving 2D recognition. However despite growing interest, 3D deep learning remains relatively underexplored. We believe that some of this disparity is due to the engineering challenges involved in 3D deep learning, such as efficiently processing heterogeneous data and reframing graphics operations to be differentiable. We address these challenges by introducing PyTorch3D, a library of modular, efficient, and differentiable operators for 3D deep learning. It includes a fast, modular differentiable renderer for meshes and point clouds, enabling analysis-by-synthesis approaches. Compared with other differentiable renderers, PyTorch3D is more modular and efficient, allowing users to more easily extend it while also gracefully scaling to large meshes and images. We compare the PyTorch3D operators and renderer with other implementations and demonstrate significant speed and memory improvements. We also use PyTorch3D to improve the state-of-the-art for unsupervised 3D mesh and point cloud prediction from 2D images on ShapeNet. PyTorch3D is open-source and we hope it will help accelerate research in 3D deep learning.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Graphics,Computer Science - Machine Learning},
  file = {C\:\\Users\\18317\\DevSpace\\DocHub\\essay\\zotero\\attachments\\gs_spalt\\Ravi et al_2020_Accelerating 3D Deep Learning with PyTorch3D.pdf;C\:\\Users\\18317\\Zotero\\storage\\7JCB8ISJ\\2007.html}
}

@inproceedings{reiserKilonerfSpeedingNeural2021,
  title = {Kilonerf: {{Speeding}} up Neural Radiance Fields with Thousands of Tiny Mlps},
  shorttitle = {Kilonerf},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF}} International Conference on Computer Vision},
  author = {Reiser, Christian and Peng, Songyou and Liao, Yiyi and Geiger, Andreas},
  date = {2021},
  pages = {14335--14345},
  url = {http://openaccess.thecvf.com/content/ICCV2021/html/Reiser_KiloNeRF_Speeding_Up_Neural_Radiance_Fields_With_Thousands_of_Tiny_ICCV_2021_paper.html},
  urldate = {2024-09-26},
  keywords = {⛔ No DOI found},
  file = {C\:\\Users\\18317\\DevSpace\\DocHub\\essay\\zotero\\attachments\\gs_spalt\\relate_works\\Reiser et al_2021_Kilonerf.pdf;C\:\\Users\\18317\\Zotero\\storage\\6MJ86ICR\\Reiser et al. - 2021 - Kilonerf Speeding up neural radiance fields with .pdf}
}

@article{renColorPointCloud2021,
  title = {Color Point Cloud Registration Algorithm Based on Hue},
  author = {Ren, Siyu and Chen, Xiaodong and Cai, Huaiyu and Wang, Yi and Liang, Haitao and Li, Haotian},
  date = {2021},
  journaltitle = {Applied sciences},
  volume = {11},
  number = {12},
  pages = {5431},
  publisher = {MDPI},
  doi = {10.3390/app11125431},
  file = {C\:\\Users\\18317\\DevSpace\\DocHub\\essay\\zotero\\attachments\\raw\\Ren et al_2021_Color point cloud registration algorithm based on hue.pdf;C\:\\Users\\18317\\Zotero\\storage\\CUHTNK4J\\Ren et al_2021_Color point cloud registration algorithm based on hue-dual-translated.pdf;C\:\\Users\\18317\\Zotero\\storage\\YPLTRIZB\\5431.html}
}

@article{renRobustGICPBased3D2019,
  title = {Robust {{GICP-Based 3D LiDAR SLAM}} for {{Underground Mining Environment}}},
  author = {Ren, Zhuli and Wang, Liguan and Bi, Lin},
  date = {2019-01},
  journaltitle = {Sensors},
  volume = {19},
  number = {13},
  pages = {2915},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {1424-8220},
  doi = {10.3390/s19132915},
  url = {https://www.mdpi.com/1424-8220/19/13/2915},
  urldate = {2024-05-22},
  abstract = {Unmanned mining is one of the most effective methods to solve mine safety and low efficiency. However, it is the key to accurate localization and mapping for underground mining environment. A novel graph simultaneous localization and mapping (SLAM) optimization method is proposed, which is based on Generalized Iterative Closest Point (GICP) three-dimensional (3D) point cloud registration between consecutive frames, between consecutive key frames and between loop frames, and is constrained by roadway plane and loop. GICP-based 3D point cloud registration between consecutive frames and consecutive key frames is first combined to optimize laser odometer constraints without other sensors such as inertial measurement unit (IMU). According to the characteristics of the roadway, the innovative extraction of the roadway plane as the node constraint of pose graph SLAM, in addition to automatic removing the noise point cloud to further improve the consistency of the underground roadway map. A lightweight and efficient loop detection and optimization based on rules and GICP is designed. Finally, the proposed method was evaluated in four scenes (such as the underground mine laboratory), and compared with the existing 3D laser SLAM method (such as Lidar Odometry and Mapping (LOAM)). The results show that the algorithm could realize low drift localization and point cloud map construction. This method provides technical support for localization and navigation of underground mining environment.},
  issue = {13},
  langid = {english},
  keywords = {GICP,graph optimization,loop detection,roadway plane,SLAM,underground mine},
  file = {C:\Users\18317\DevSpace\DocHub\essay\zotero\attachments\raw\Ren et al_2019_Robust GICP-Based 3D LiDAR SLAM for Underground Mining Environment.pdf}
}

@online{ressSLAMIndoorMapping2024,
  title = {{{SLAM}} for {{Indoor Mapping}} of {{Wide Area Construction Environments}}},
  author = {Ress, Vincent and Zhang, Wei and Skuddis, David and Haala, Norbert and Soergel, Uwe},
  date = {2024-04-26},
  eprint = {2404.17215},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2404.17215},
  url = {http://arxiv.org/abs/2404.17215},
  urldate = {2024-05-20},
  abstract = {Simultaneous localization and mapping (SLAM), i.e., the reconstruction of the environment represented by a (3D) map and the concurrent pose estimation, has made astonishing progress. Meanwhile, large scale applications aiming at the data collection in complex environments like factory halls or construction sites are becoming feasible. However, in contrast to small scale scenarios with building interiors separated to single rooms, shop floors or construction areas require measures at larger distances in potentially texture less areas under difficult illumination. Pose estimation is further aggravated since no GNSS measures are available as it is usual for such indoor applications. In our work, we realize data collection in a large factory hall by a robot system equipped with four stereo cameras as well as a 3D laser scanner. We apply our state-of-the-art LiDAR and visual SLAM approaches and discuss the respective pros and cons of the different sensor types for trajectory estimation and dense map generation in such an environment. Additionally, dense and accurate depth maps are generated by 3D Gaussian splatting, which we plan to use in the context of our project aiming on the automatic construction and site monitoring.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Robotics},
  file = {C\:\\Users\\18317\\DevSpace\\DocHub\\essay\\zotero\\attachments\\SLAM\\Ress_et_al_2024_SLAM_for_Indoor_Mapping_of_Wide_Area_Construction_Environments.pdf;C\:\\Users\\18317\\Zotero\\storage\\Y845EHIN\\2404.html}
}

@inproceedings{rusinkiewiczEfficientVariantsICP2001,
  title = {Efficient Variants of the {{ICP}} Algorithm},
  booktitle = {Proceedings Third International Conference on 3-{{D}} Digital Imaging and Modeling},
  author = {Rusinkiewicz, Szymon and Levoy, Marc},
  date = {2001},
  pages = {145--152},
  publisher = {IEEE},
  doi = {10.1109/IM.2001.924423},
  url = {https://ieeexplore.ieee.org/abstract/document/924423/},
  urldate = {2024-09-26},
  file = {C:\Users\18317\DevSpace\DocHub\essay\zotero\attachments\gs_spalt\introduce\Rusinkiewicz_Levoy_2001_Efficient variants of the ICP algorithm.pdf}
}

@article{sahilliogluScaleadaptiveICP2021,
  title = {Scale-Adaptive {{ICP}}},
  author = {Sahillioğlu, Yusuf and Kavan, Ladislav},
  date = {2021},
  journaltitle = {Graphical Models},
  volume = {116},
  pages = {101113},
  publisher = {Elsevier},
  doi = {10.1016/j.gmod.2021.101113},
  url = {https://www.sciencedirect.com/science/article/pii/S1524070321000187},
  urldate = {2024-06-03},
  keywords = {多尺度对齐},
  file = {C:\Users\18317\DevSpace\DocHub\essay\zotero\attachments\raw\Sahillioğlu_Kavan_2021_Scale-adaptive ICP.pdf}
}

@inproceedings{sandstromPointslamDenseNeural2023,
  title = {Point-Slam: {{Dense}} Neural Point Cloud-Based Slam},
  shorttitle = {Point-Slam},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF International Conference}} on {{Computer Vision}}},
  author = {Sandström, Erik and Li, Yue and Van Gool, Luc and Oswald, Martin R.},
  date = {2023},
  pages = {18433--18444},
  url = {http://openaccess.thecvf.com/content/ICCV2023/html/Sandstrom_Point-SLAM_Dense_Neural_Point_Cloud-based_SLAM_ICCV_2023_paper.html},
  urldate = {2024-08-22},
  keywords = {⛔ No DOI found},
  file = {C:\Users\18317\DevSpace\DocHub\essay\zotero\attachments\gs_spalt\introduce\Sandström et al_2023_Point-slam.pdf}
}

@inproceedings{sarlinBackFeatureLearning2021,
  title = {Back to the {{Feature}}: {{Learning Robust Camera Localization}} from {{Pixels}} to {{Pose}}},
  shorttitle = {Back to the Feature},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF}} Conference on Computer Vision and Pattern Recognition},
  author = {Sarlin, Paul-Edouard and Unagar, Ajaykumar and Larsson, Mans and Germain, Hugo and Toft, Carl and Larsson, Viktor and Pollefeys, Marc and Lepetit, Vincent and Hammarstrand, Lars and Kahl, Fredrik},
  date = {2021},
  pages = {3247--3257},
  url = {http://openaccess.thecvf.com/content/CVPR2021/html/Sarlin_Back_to_the_Feature_Learning_Robust_Camera_Localization_From_Pixels_CVPR_2021_paper.html},
  urldate = {2024-06-15},
  keywords = {⛔ No DOI found},
  file = {C:\Users\18317\DevSpace\DocHub\essay\zotero\attachments\gs_spalt\Sarlin et al_2021_Back to the feature2.pdf}
}

@article{scaramuzzaVisualOdometryTutorial2011,
  title = {Visual Odometry [Tutorial]},
  author = {Scaramuzza, Davide and Fraundorfer, Friedrich},
  date = {2011},
  journaltitle = {IEEE robotics \& automation magazine},
  volume = {18},
  number = {4},
  pages = {80--92},
  publisher = {IEEE},
  doi = {10.1109/MRA.2011.943233},
  url = {https://ieeexplore.ieee.org/abstract/document/6096039/},
  urldate = {2024-09-26},
  file = {C:\Users\18317\DevSpace\DocHub\essay\zotero\attachments\gs_spalt\introduce\Scaramuzza_Fraundorfer_2011_Visual odometry [tutorial].pdf}
}

@inproceedings{schonbergerStructurefrommotionRevisited2016,
  title = {Structure-from-Motion Revisited},
  booktitle = {Proceedings of the {{IEEE}} Conference on Computer Vision and Pattern Recognition},
  author = {Schonberger, Johannes L. and Frahm, Jan-Michael},
  date = {2016},
  pages = {4104--4113},
  url = {https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/Schonberger_Structure-From-Motion_Revisited_CVPR_2016_paper.html},
  urldate = {2024-06-15},
  keywords = {⛔ No DOI found},
  file = {C:\Users\18317\DevSpace\DocHub\essay\zotero\attachments\gs_spalt\Schonberger_Frahm_2016_Structure-from-motion revisited2.pdf}
}

@inproceedings{schopsBadSlamBundle2019,
  title = {Bad Slam: {{Bundle}} Adjusted Direct Rgb-d Slam},
  shorttitle = {Bad Slam},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Schops, Thomas and Sattler, Torsten and Pollefeys, Marc},
  date = {2019},
  pages = {134--144},
  url = {http://openaccess.thecvf.com/content_CVPR_2019/html/Schops_BAD_SLAM_Bundle_Adjusted_Direct_RGB-D_SLAM_CVPR_2019_paper.html},
  urldate = {2024-09-03},
  keywords = {⛔ No DOI found},
  file = {C:\Users\18317\DevSpace\DocHub\essay\zotero\attachments\gs_spalt\relate_works\Schops et al_2019_Bad slam.pdf}
}

@inproceedings{segalGeneralizedicp2009a,
  title = {Generalized-Icp.},
  booktitle = {Robotics: Science and Systems},
  author = {Segal, Aleksandr and Haehnel, Dirk and Thrun, Sebastian},
  date = {2009},
  volume = {2},
  number = {4},
  pages = {435},
  publisher = {Seattle, WA},
  doi = {10.15607/RSS.2009.V.021},
  url = {https://direct.mit.edu/books/edited-volume/chapter-pdf/2277340/9780262289801_cau.pdf},
  urldate = {2024-05-20},
  file = {C:\Users\18317\DevSpace\DocHub\essay\zotero\attachments\EMPTY_COLLECTION_NAMESegal et al\Segal et al_2009_Generalized-icp.pdf}
}

@online{smithFlowMapHighQualityCamera2024,
  title = {{{FlowMap}}: {{High-Quality Camera Poses}}, {{Intrinsics}}, and {{Depth}} via {{Gradient Descent}}},
  shorttitle = {{{FlowMap}}},
  author = {Smith, Cameron and Charatan, David and Tewari, Ayush and Sitzmann, Vincent},
  date = {2024-04-23},
  eprint = {2404.15259},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2404.15259},
  urldate = {2024-06-12},
  abstract = {This paper introduces FlowMap, an end-to-end differentiable method that solves for precise camera poses, camera intrinsics, and per-frame dense depth of a video sequence. Our method performs per-video gradient-descent minimization of a simple least-squares objective that compares the optical flow induced by depth, intrinsics, and poses against correspondences obtained via off-the-shelf optical flow and point tracking. Alongside the use of point tracks to encourage long-term geometric consistency, we introduce differentiable re-parameterizations of depth, intrinsics, and pose that are amenable to first-order optimization. We empirically show that camera parameters and dense depth recovered by our method enable photo-realistic novel view synthesis on 360-degree trajectories using Gaussian Splatting. Our method not only far outperforms prior gradient-descent based bundle adjustment methods, but surprisingly performs on par with COLMAP, the state-of-the-art SfM method, on the downstream task of 360-degree novel view synthesis (even though our method is purely gradient-descent based, fully differentiable, and presents a complete departure from conventional SfM).},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\18317\\Zotero\\storage\\JS3WVP2A\\Smith et al. - 2024 - FlowMap High-Quality Camera Poses, Intrinsics, an.pdf;C\:\\Users\\18317\\Zotero\\storage\\LN4YMZKV\\2404.html}
}

@unpublished{sonWGICPDifferentiableWeighted2022,
  title = {{{WGICP}}: {{Differentiable Weighted GICP-Based Lidar Odometry}}},
  shorttitle = {{{WGICP}}},
  author = {Son, Sanghyun and Liang, Jing and Lin, Ming and Manocha, Dinesh},
  date = {2022},
  eprint = {2209.09777},
  eprinttype = {arXiv},
  keywords = {⛔ No DOI found},
  file = {C\:\\Users\\18317\\DevSpace\\DocHub\\essay\\zotero\\attachments\\raw\\Son et al_2022_WGICP.pdf;C\:\\Users\\18317\\Zotero\\storage\\Q3TLWDVF\\Son et al_2022_WGICP-dual-translated.pdf}
}

@inproceedings{steinbruckerRealtimeVisualOdometry2011,
  title = {Real-Time Visual Odometry from Dense {{RGB-D}} Images},
  booktitle = {2011 {{IEEE}} International Conference on Computer Vision Workshops ({{ICCV Workshops}})},
  author = {Steinbrücker, Frank and Sturm, Jürgen and Cremers, Daniel},
  date = {2011},
  pages = {719--722},
  publisher = {IEEE},
  doi = {10.1109/ICCVW.2011.6130321},
  url = {https://ieeexplore.ieee.org/abstract/document/6130321/},
  urldate = {2024-08-20},
  keywords = {nosource}
}

@online{straubReplicaDatasetDigital2019,
  title = {The {{Replica Dataset}}: {{A Digital Replica}} of {{Indoor Spaces}}},
  shorttitle = {The {{Replica Dataset}}},
  author = {Straub, Julian and Whelan, Thomas and Ma, Lingni and Chen, Yufan and Wijmans, Erik and Green, Simon and Engel, Jakob J. and Mur-Artal, Raul and Ren, Carl and Verma, Shobhit and Clarkson, Anton and Yan, Mingfei and Budge, Brian and Yan, Yajie and Pan, Xiaqing and Yon, June and Zou, Yuyang and Leon, Kimberly and Carter, Nigel and Briales, Jesus and Gillingham, Tyler and Mueggler, Elias and Pesqueira, Luis and Savva, Manolis and Batra, Dhruv and Strasdat, Hauke M. and De Nardi, Renzo and Goesele, Michael and Lovegrove, Steven and Newcombe, Richard},
  date = {2019-06-13},
  eprint = {1906.05797},
  eprinttype = {arXiv},
  eprintclass = {cs, eess},
  url = {http://arxiv.org/abs/1906.05797},
  urldate = {2024-08-10},
  abstract = {We introduce Replica, a dataset of 18 highly photo-realistic 3D indoor scene reconstructions at room and building scale. Each scene consists of a dense mesh, high-resolution high-dynamic-range (HDR) textures, per-primitive semantic class and instance information, and planar mirror and glass reflectors. The goal of Replica is to enable machine learning (ML) research that relies on visually, geometrically, and semantically realistic generative models of the world - for instance, egocentric computer vision, semantic segmentation in 2D and 3D, geometric inference, and the development of embodied agents (virtual robots) performing navigation, instruction following, and question answering. Due to the high level of realism of the renderings from Replica, there is hope that ML systems trained on Replica may transfer directly to real world image and video data. Together with the data, we are releasing a minimal C++ SDK as a starting point for working with the Replica dataset. In addition, Replica is `Habitat-compatible', i.e. can be natively used with AI Habitat for training and testing embodied agents.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Graphics,Electrical Engineering and Systems Science - Image and Video Processing},
  file = {C\:\\Users\\18317\\DevSpace\\DocHub\\essay\\zotero\\attachments\\gs_spalt\\experiments\\Straub et al_2019_The Replica Dataset.pdf;C\:\\Users\\18317\\Zotero\\storage\\VRTGI9PS\\1906.html}
}

@inproceedings{sturmBenchmarkEvaluationRGBD2012,
  title = {A Benchmark for the Evaluation of {{RGB-D SLAM}} Systems},
  booktitle = {2012 {{IEEE}}/{{RSJ}} International Conference on Intelligent Robots and Systems},
  author = {Sturm, Jürgen and Engelhard, Nikolas and Endres, Felix and Burgard, Wolfram and Cremers, Daniel},
  date = {2012},
  pages = {573--580},
  publisher = {IEEE},
  doi = {10.1109/IROS.2012.6385773},
  url = {https://ieeexplore.ieee.org/abstract/document/6385773/},
  urldate = {2024-08-10},
  file = {C:\Users\18317\DevSpace\DocHub\essay\zotero\attachments\gs_spalt\experiments\Sturm et al_2012_A benchmark for the evaluation of RGB-D SLAM systems.pdf}
}

@inproceedings{sucarImapImplicitMapping2021,
  title = {Imap: {{Implicit}} Mapping and Positioning in Real-Time},
  shorttitle = {Imap},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF}} International Conference on Computer Vision},
  author = {Sucar, Edgar and Liu, Shikun and Ortiz, Joseph and Davison, Andrew J.},
  date = {2021},
  pages = {6229--6238},
  url = {http://openaccess.thecvf.com/content/ICCV2021/html/Sucar_iMAP_Implicit_Mapping_and_Positioning_in_Real-Time_ICCV_2021_paper.html},
  urldate = {2024-08-10},
  keywords = {⛔ No DOI found},
  file = {C:\Users\18317\DevSpace\DocHub\essay\zotero\attachments\gs_spalt\experiments\Sucar et al_2021_imap.pdf}
}

@inproceedings{sucarImapImplicitMapping2021a,
  title = {Imap: {{Implicit}} Mapping and Positioning in Real-Time},
  shorttitle = {Imap},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF}} International Conference on Computer Vision},
  author = {Sucar, Edgar and Liu, Shikun and Ortiz, Joseph and Davison, Andrew J.},
  date = {2021},
  pages = {6229--6238},
  url = {http://openaccess.thecvf.com/content/ICCV2021/html/Sucar_iMAP_Implicit_Mapping_and_Positioning_in_Real-Time_ICCV_2021_paper.html},
  urldate = {2024-09-26},
  keywords = {⛔ No DOI found},
  file = {C:\Users\18317\DevSpace\DocHub\essay\zotero\attachments\gs_spalt\relate_works\Sucar et al_2021_imap.pdf}
}

@inproceedings{sunDirectVoxelGrid2022,
  title = {Direct Voxel Grid Optimization: {{Super-fast}} Convergence for Radiance Fields Reconstruction},
  shorttitle = {Direct Voxel Grid Optimization},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF}} Conference on Computer Vision and Pattern Recognition},
  author = {Sun, Cheng and Sun, Min and Chen, Hwann-Tzong},
  date = {2022},
  pages = {5459--5469},
  url = {http://openaccess.thecvf.com/content/CVPR2022/html/Sun_Direct_Voxel_Grid_Optimization_Super-Fast_Convergence_for_Radiance_Fields_Reconstruction_CVPR_2022_paper.html},
  urldate = {2024-09-03},
  keywords = {⛔ No DOI found},
  file = {C:\Users\18317\DevSpace\DocHub\essay\zotero\attachments\gs_spalt\relate_works\Sun et al_2022_Direct voxel grid optimization.pdf}
}

@online{sunMM3DGSSLAMMultimodal2024,
  title = {{{MM3DGS SLAM}}: {{Multi-modal 3D Gaussian Splatting}} for {{SLAM Using Vision}}, {{Depth}}, and {{Inertial Measurements}}},
  shorttitle = {{{MM3DGS SLAM}}},
  author = {Sun, Lisong C. and Bhatt, Neel P. and Liu, Jonathan C. and Fan, Zhiwen and Wang, Zhangyang and Humphreys, Todd E. and Topcu, Ufuk},
  date = {2024-04-01},
  eprint = {2404.00923},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2404.00923},
  urldate = {2024-07-16},
  abstract = {Simultaneous localization and mapping is essential for position tracking and scene understanding. 3D Gaussian-based map representations enable photorealistic reconstruction and real-time rendering of scenes using multiple posed cameras. We show for the first time that using 3D Gaussians for map representation with unposed camera images and inertial measurements can enable accurate SLAM. Our method, MM3DGS, addresses the limitations of prior neural radiance field-based representations by enabling faster rendering, scale awareness, and improved trajectory tracking. Our framework enables keyframe-based mapping and tracking utilizing loss functions that incorporate relative pose transformations from pre-integrated inertial measurements, depth estimates, and measures of photometric rendering quality. We also release a multi-modal dataset, UT-MM, collected from a mobile robot equipped with a camera and an inertial measurement unit. Experimental evaluation on several scenes from the dataset shows that MM3DGS achieves 3x improvement in tracking and 5\% improvement in photometric rendering quality compared to the current 3DGS SLAM state-of-the-art, while allowing real-time rendering of a high-resolution dense 3D map. Project Webpage: https://vita-group.github.io/MM3DGS-SLAM},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Robotics},
  file = {C\:\\Users\\18317\\DevSpace\\DocHub\\essay\\zotero\\attachments\\gs_spalt\\Sun et al_2024_MM3DGS SLAM2.pdf;C\:\\Users\\18317\\Zotero\\storage\\DEZTQSC3\\2404.html}
}

@online{tangDreamGaussianGenerativeGaussian2024,
  title = {{{DreamGaussian}}: {{Generative Gaussian Splatting}} for {{Efficient 3D Content Creation}}},
  shorttitle = {{{DreamGaussian}}},
  author = {Tang, Jiaxiang and Ren, Jiawei and Zhou, Hang and Liu, Ziwei and Zeng, Gang},
  date = {2024-03-29},
  eprint = {2309.16653},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2309.16653},
  urldate = {2024-09-03},
  abstract = {Recent advances in 3D content creation mostly leverage optimization-based 3D generation via score distillation sampling (SDS). Though promising results have been exhibited, these methods often suffer from slow per-sample optimization, limiting their practical usage. In this paper, we propose DreamGaussian, a novel 3D content generation framework that achieves both efficiency and quality simultaneously. Our key insight is to design a generative 3D Gaussian Splatting model with companioned mesh extraction and texture refinement in UV space. In contrast to the occupancy pruning used in Neural Radiance Fields, we demonstrate that the progressive densification of 3D Gaussians converges significantly faster for 3D generative tasks. To further enhance the texture quality and facilitate downstream applications, we introduce an efficient algorithm to convert 3D Gaussians into textured meshes and apply a fine-tuning stage to refine the details. Extensive experiments demonstrate the superior efficiency and competitive generation quality of our proposed approach. Notably, DreamGaussian produces high-quality textured meshes in just 2 minutes from a single-view image, achieving approximately 10 times acceleration compared to existing methods.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\18317\\DevSpace\\DocHub\\essay\\zotero\\attachments\\gs_spalt\\relate_works\\Tang et al_2024_DreamGaussian.pdf;C\:\\Users\\18317\\Zotero\\storage\\KL5GARN4\\2309.html}
}

@article{tausskySimilarityTransformationMatirx1959,
  title = {On the Similarity Transformation between a Matirx and Its Transpose.},
  author = {Taussky, Olga and Zassenhaus, Hans},
  date = {1959},
  doi = {10.2140/pjm.1959.9.893},
  url = {https://msp.org/pjm/1959/9-3/pjm-v9-n3-p.pdf#page=267},
  urldate = {2024-07-23},
  file = {C:\Users\18317\DevSpace\DocHub\essay\zotero\attachments\gs_spalt\method\Taussky_Zassenhaus_1959_On the similarity transformation between a matirx and its transpose.pdf}
}

@article{teedDroidslamDeepVisual2021,
  title = {Droid-Slam: {{Deep}} Visual Slam for Monocular, Stereo, and Rgb-d Cameras},
  shorttitle = {Droid-Slam},
  author = {Teed, Zachary and Deng, Jia},
  date = {2021},
  journaltitle = {Advances in neural information processing systems},
  volume = {34},
  pages = {16558--16569},
  url = {https://proceedings.neurips.cc/paper/2021/hash/89fcd07f20b6785b92134bd6c1d0fa42-Abstract.html},
  urldate = {2024-06-15},
  keywords = {⛔ No DOI found},
  file = {C:\Users\18317\DevSpace\DocHub\essay\zotero\attachments\gs_spalt\Teed_Deng_2021_Droid-slam2.pdf}
}

@article{teedDroidslamDeepVisual2021a,
  title = {Droid-Slam: {{Deep}} Visual Slam for Monocular, Stereo, and Rgb-d Cameras},
  shorttitle = {Droid-Slam},
  author = {Teed, Zachary and Deng, Jia},
  date = {2021},
  journaltitle = {Advances in neural information processing systems},
  volume = {34},
  pages = {16558--16569},
  url = {https://proceedings.neurips.cc/paper/2021/hash/89fcd07f20b6785b92134bd6c1d0fa42-Abstract.html},
  urldate = {2024-09-05},
  keywords = {⛔ No DOI found},
  file = {C:\Users\18317\DevSpace\DocHub\essay\zotero\attachments\gs_spalt\relate_works\Teed_Deng_2021_Droid-slam.pdf}
}

@article{torrobaPointNetKLDeepInference2020,
  title = {{{PointNetKL}}: {{Deep}} Inference for {{GICP}} Covariance Estimation in Bathymetric {{SLAM}}},
  shorttitle = {{{PointNetKL}}},
  author = {Torroba, Ignacio and Sprague, Christopher Iliffe and Bore, Nils and Folkesson, John},
  date = {2020},
  journaltitle = {IEEE Robotics and Automation Letters},
  volume = {5},
  number = {3},
  pages = {4078--4085},
  publisher = {IEEE},
  doi = {10.1109/LRA.2020.2988180},
  url = {https://ieeexplore.ieee.org/abstract/document/9072324/},
  urldate = {2024-06-03},
  file = {C:\Users\18317\DevSpace\DocHub\essay\zotero\attachments\raw\Torroba et al_2020_PointNetKL.pdf}
}

@article{tsintotasTrackingDOSeqSLAMDynamicSequencebased2021,
  title = {Tracking‐{{DOSeqSLAM}}: {{A}} Dynamic Sequence‐based Visual Place Recognition Paradigm},
  shorttitle = {Tracking‐{{DOSeqSLAM}}},
  author = {Tsintotas, Konstantinos A. and Bampis, Loukas and Gasteratos, Antonios and {FIET}},
  date = {2021-06},
  journaltitle = {IET Computer Vision},
  volume = {15},
  number = {4},
  pages = {258--273},
  publisher = {{Institution of Engineering and Technology (IET)}},
  issn = {1751-9632, 1751-9640},
  doi = {10.1049/cvi2.12041},
  url = {https://onlinelibrary.wiley.com/doi/10.1049/cvi2.12041},
  urldate = {2024-09-17},
  langid = {english},
  file = {C:\Users\18317\DevSpace\DocHub\essay\zotero\attachments\raw\Tsintotas et al_2021_Tracking‐DOSeqSLAM.pdf}
}

@inproceedings{tulsianiMultiviewSupervisionSingleview2017,
  title = {Multi-View Supervision for Single-View Reconstruction via Differentiable Ray Consistency},
  booktitle = {Proceedings of the {{IEEE}} Conference on Computer Vision and Pattern Recognition},
  author = {Tulsiani, Shubham and Zhou, Tinghui and Efros, Alexei A. and Malik, Jitendra},
  date = {2017},
  pages = {2626--2634},
  url = {http://openaccess.thecvf.com/content_cvpr_2017/html/Tulsiani_Multi-View_Supervision_for_CVPR_2017_paper.html},
  urldate = {2024-09-26},
  keywords = {⛔ No DOI found},
  file = {C:\Users\18317\DevSpace\DocHub\essay\zotero\attachments\gs_spalt\relate_works\Tulsiani et al_2017_Multi-view supervision for single-view reconstruction via differentiable ray.pdf}
}

@article{vizzoKissicpDefensePointtopoint2023,
  title = {Kiss-Icp: {{In}} Defense of Point-to-Point Icp–Simple, Accurate, and Robust Registration If Done the Right Way},
  shorttitle = {Kiss-Icp},
  author = {Vizzo, Ignacio and Guadagnino, Tiziano and Mersch, Benedikt and Wiesmann, Louis and Behley, Jens and Stachniss, Cyrill},
  date = {2023},
  journaltitle = {IEEE Robotics and Automation Letters},
  volume = {8},
  number = {2},
  pages = {1029--1036},
  publisher = {IEEE},
  doi = {10.1109/LRA.2023.3236571},
  url = {https://ieeexplore.ieee.org/abstract/document/10015694/},
  urldate = {2024-06-03},
  keywords = {多尺度对齐},
  file = {C:\Users\18317\DevSpace\DocHub\essay\zotero\attachments\raw\Vizzo et al_2023_Kiss-icp.pdf}
}

@incollection{wangImprovingRGBDPoint2022,
  title = {Improving {{RGB-D Point Cloud Registration}} by {{Learning Multi-scale Local Linear Transformation}}},
  booktitle = {Computer {{Vision}} – {{ECCV}} 2022},
  author = {Wang, Ziming and Huo, Xiaoliang and Chen, Zhenghao and Zhang, Jing and Sheng, Lu and Xu, Dong},
  editor = {Avidan, Shai and Brostow, Gabriel and Cissé, Moustapha and Farinella, Giovanni Maria and Hassner, Tal},
  date = {2022},
  volume = {13692},
  pages = {175--191},
  publisher = {Springer Nature Switzerland},
  location = {Cham},
  doi = {10.1007/978-3-031-19824-3_11},
  url = {https://link.springer.com/10.1007/978-3-031-19824-3_11},
  urldate = {2024-06-05},
  isbn = {978-3-031-19823-6 978-3-031-19824-3},
  langid = {english},
  file = {C:\Users\18317\DevSpace\DocHub\essay\zotero\attachments\raw\Wang et al_2022_Improving RGB-D Point Cloud Registration by Learning Multi-scale Local Linear2.pdf}
}

@online{wangVoGEDifferentiableVolume2024,
  title = {{{VoGE}}: {{A Differentiable Volume Renderer}} Using {{Gaussian Ellipsoids}} for {{Analysis-by-Synthesis}}},
  shorttitle = {{{VoGE}}},
  author = {Wang, Angtian and Wang, Peng and Sun, Jian and Kortylewski, Adam and Yuille, Alan},
  date = {2024-01-28},
  eprint = {2205.15401},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2205.15401},
  urldate = {2024-09-03},
  abstract = {The Gaussian reconstruction kernels have been proposed by Westover (1990) and studied by the computer graphics community back in the 90s, which gives an alternative representation of object 3D geometry from meshes and point clouds. On the other hand, current state-of-the-art (SoTA) differentiable renderers, Liu et al. (2019), use rasterization to collect triangles or points on each image pixel and blend them based on the viewing distance. In this paper, we propose VoGE, which utilizes the volumetric Gaussian reconstruction kernels as geometric primitives. The VoGE rendering pipeline uses ray tracing to capture the nearest primitives and blends them as mixtures based on their volume density distributions along the rays. To efficiently render via VoGE, we propose an approximate closeform solution for the volume density aggregation and a coarse-to-fine rendering strategy. Finally, we provide a CUDA implementation of VoGE, which enables real-time level rendering with a competitive rendering speed in comparison to PyTorch3D. Quantitative and qualitative experiment results show VoGE outperforms SoTA counterparts when applied to various vision tasks, e.g., object pose estimation, shape/texture fitting, and occlusion reasoning. The VoGE library and demos are available at: https://github.com/Angtian/VoGE.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Graphics},
  file = {C\:\\Users\\18317\\DevSpace\\DocHub\\essay\\zotero\\attachments\\gs_spalt\\relate_works\\Wang et al_2024_VoGE.pdf;C\:\\Users\\18317\\Zotero\\storage\\K67BN66M\\2205.html}
}

@inproceedings{whelanElasticFusionDenseSLAM2015,
  title = {{{ElasticFusion}}: {{Dense SLAM}} without a Pose Graph.},
  shorttitle = {{{ElasticFusion}}},
  booktitle = {Robotics: Science and Systems},
  author = {Whelan, Thomas and Leutenegger, Stefan and Salas-Moreno, Renato F. and Glocker, Ben and Davison, Andrew J.},
  date = {2015},
  volume = {11},
  pages = {3},
  publisher = {Rome, Italy},
  doi = {10.15607/RSS.2015.XI.001},
  url = {https://roboticsproceedings.org/rss11/p01.pdf},
  urldate = {2024-09-02},
  file = {C:\Users\18317\DevSpace\DocHub\essay\zotero\attachments\gs_spalt\relate_works\Whelan et al_2015_ElasticFusion.pdf}
}

@article{whelanElasticFusionRealtimeDense2016,
  title = {{{ElasticFusion}}: {{Real-time}} Dense {{SLAM}} and Light Source Estimation},
  shorttitle = {{{ElasticFusion}}},
  author = {Whelan, Thomas and Salas-Moreno, Renato F and Glocker, Ben and Davison, Andrew J and Leutenegger, Stefan},
  date = {2016-12},
  journaltitle = {The International Journal of Robotics Research},
  shortjournal = {The International Journal of Robotics Research},
  volume = {35},
  number = {14},
  pages = {1697--1716},
  issn = {0278-3649, 1741-3176},
  doi = {10.1177/0278364916669237},
  url = {http://journals.sagepub.com/doi/10.1177/0278364916669237},
  urldate = {2024-09-13},
  abstract = {We present a novel approach to real-time dense visual simultaneous localisation and mapping. Our system is capable of capturing comprehensive dense globally consistent surfel-based maps of room scale environments and beyond explored using an RGB-D camera in an incremental online fashion, without pose graph optimization or any post-processing steps. This is accomplished by using dense frame-to-model camera tracking and windowed surfel-based fusion coupled with frequent model refinement through non-rigid surface deformations. Our approach applies local model-to-model surface loop closure optimizations as often as possible to stay close to the mode of the map distribution, while utilizing global loop closure to recover from arbitrary drift and maintain global consistency. In the spirit of improving map quality as well as tracking accuracy and robustness, we furthermore explore a novel approach to real-time discrete light source detection. This technique is capable of detecting numerous light sources in indoor environments in real-time as a user handheld camera explores the scene. Absolutely no prior information about the scene or number of light sources is required. By making a small set of simple assumptions about the appearance properties of the scene our method can incrementally estimate both the quantity and location of multiple light sources in the environment in an online fashion. Our results demonstrate that our technique functions well in many different environments and lighting configurations. We show that this enables (a) more realistic augmented reality rendering; (b) a richer understanding of the scene beyond pure geometry and; (c) more accurate and robust photometric tracking.},
  langid = {english},
  file = {C\:\\Users\\18317\\DevSpace\\DocHub\\essay\\zotero\\attachments\\gs_spalt\\relate_works\\Whelan et al_2016_ElasticFusion.pdf;C\:\\Users\\18317\\DevSpace\\DocHub\\essay\\zotero\\attachments\\gs_spalt\\relate_works\\Whelan et al_2016_ElasticFusion2.pdf}
}

@article{whelanRealtimeLargescaleDense2015,
  title = {Real-Time Large-Scale Dense {{RGB-D SLAM}} with Volumetric Fusion},
  author = {Whelan, Thomas and Kaess, Michael and Johannsson, Hordur and Fallon, Maurice and Leonard, John J. and McDonald, John},
  date = {2015-04},
  journaltitle = {The International Journal of Robotics Research},
  shortjournal = {The International Journal of Robotics Research},
  volume = {34},
  number = {4-5},
  pages = {598--626},
  issn = {0278-3649, 1741-3176},
  doi = {10.1177/0278364914551008},
  url = {http://journals.sagepub.com/doi/10.1177/0278364914551008},
  urldate = {2024-09-03},
  abstract = {We present a new simultaneous localization and mapping (SLAM) system capable of producing high-quality globally consistent surface reconstructions over hundreds of meters in real time with only a low-cost commodity RGB-D sensor. By using a fused volumetric surface reconstruction we achieve a much higher quality map over what would be achieved using raw RGB-D point clouds. In this paper we highlight three key techniques associated with applying a volumetric fusion-based mapping system to the SLAM problem in real time. First, the use of a GPU-based 3D cyclical buffer trick to efficiently extend dense every-frame volumetric fusion of depth maps to function over an unbounded spatial region. Second, overcoming camera pose estimation limitations in a wide variety of environments by combining both dense geometric and photometric camera pose constraints. Third, efficiently updating the dense map according to place recognition and subsequent loop closure constraints by the use of an ‘as-rigid-as-possible’ space deformation. We present results on a wide variety of aspects of the system and show through evaluation on de facto standard RGB-D benchmarks that our system performs strongly in terms of trajectory estimation, map quality and computational performance in comparison to other state-of-the-art systems.},
  langid = {english},
  file = {C:\Users\18317\DevSpace\DocHub\essay\zotero\attachments\gs_spalt\relate_works\Whelan et al_2015_Real-time large-scale dense RGB-D SLAM with volumetric fusion.pdf}
}

@inproceedings{wu4dGaussianSplatting2024,
  title = {4d Gaussian Splatting for Real-Time Dynamic Scene Rendering},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Wu, Guanjun and Yi, Taoran and Fang, Jiemin and Xie, Lingxi and Zhang, Xiaopeng and Wei, Wei and Liu, Wenyu and Tian, Qi and Wang, Xinggang},
  date = {2024},
  pages = {20310--20320},
  url = {https://openaccess.thecvf.com/content/CVPR2024/html/Wu_4D_Gaussian_Splatting_for_Real-Time_Dynamic_Scene_Rendering_CVPR_2024_paper.html},
  urldate = {2024-09-03},
  keywords = {⛔ No DOI found},
  file = {C:\Users\18317\DevSpace\DocHub\essay\zotero\attachments\gs_spalt\relate_works\Wu et al_2024_4d gaussian splatting for real-time dynamic scene rendering.pdf}
}

@article{xieAccurateLocalizationMoving2022,
  title = {Accurate Localization of Moving Objects in Dynamic Environment for Small Unmanned Aerial Vehicle Platform Using Global Averaging},
  author = {Xie, Xiuchuan and Yang, Tao and Zhang, Yanning and Liang, Bang and Liu, Linfeng},
  date = {2022-02},
  journaltitle = {IET Computer Vision},
  shortjournal = {IET Computer Vision},
  volume = {16},
  number = {1},
  pages = {12--25},
  issn = {1751-9632, 1751-9640},
  doi = {10.1049/cvi2.12053},
  url = {https://onlinelibrary.wiley.com/doi/10.1049/cvi2.12053},
  urldate = {2024-09-15},
  langid = {english}
}

@article{xieAccurateLocalizationMoving2022a,
  title = {Accurate Localization of Moving Objects in Dynamic Environment for Small Unmanned Aerial Vehicle Platform Using Global Averaging},
  author = {Xie, Xiuchuan and Yang, Tao and Zhang, Yanning and Liang, Bang and Liu, Linfeng},
  date = {2022-02},
  journaltitle = {IET Computer Vision},
  volume = {16},
  number = {1},
  pages = {12--25},
  publisher = {{Institution of Engineering and Technology (IET)}},
  issn = {1751-9632, 1751-9640},
  doi = {10.1049/cvi2.12053},
  url = {https://onlinelibrary.wiley.com/doi/10.1049/cvi2.12053},
  urldate = {2024-09-17},
  langid = {english}
}

@article{xuDeepLearningMultiple2019,
  title = {Deep Learning for Multiple Object Tracking: A Survey},
  shorttitle = {Deep Learning for Multiple Object Tracking},
  author = {Xu, Yingkun and Zhou, Xiaolong and Chen, Shengyong and Li, Fenfen},
  date = {2019-06},
  journaltitle = {IET Computer Vision},
  shortjournal = {IET Computer Vision},
  volume = {13},
  number = {4},
  pages = {355--368},
  issn = {1751-9632, 1751-9640},
  doi = {10.1049/iet-cvi.2018.5598},
  url = {https://ietresearch.onlinelibrary.wiley.com/doi/10.1049/iet-cvi.2018.5598},
  urldate = {2024-09-15},
  abstract = {Deep learning has been proved effective in multiple object tracking, which confronts the difficulties of frequent occlusions, confusing appearance, in‐and‐out objects, and lack of enough labelled data. Recently, deep learning based multi‐object tracking methods make a rapid progress from representation learning to network modelling due to the development of deep learning theory and benchmark setup. In this study, the authors summarise and analyse deep learning based multi‐object tracking methods which are top‐ranked in the public benchmark test. First, they investigate functionality of deep networks in these methods, and classify the methods into three categories as description enhancement using deep features, deep network embedding, and end‐to‐end deep network construction. Second, they review deep network structures in these methods, and detail the usage and training of these networks for multi‐object tracking problem. Through experimental comparison of tracking results in the benchmarks in total and by group, they finally show the effectiveness of deep networks for tracking employed in different manners, and compare the advantages of these networks and their robustness under different tracking conditions. Moreover, they analyse the limitations of current methods, and draw some useful conclusions to facilitate the exploration of new directions for multi‐object tracking.},
  langid = {english},
  file = {C\:\\Users\\18317\\DevSpace\\DocHub\\essay\\zotero\\attachments\\raw\\Xu et al_2019_Deep learning for multiple object tracking.pdf;C\:\\Users\\18317\\DevSpace\\DocHub\\essay\\zotero\\attachments\\raw\\Xu et al_2019_Deep learning for multiple object tracking2.pdf}
}

@article{yangColorPointCloud2020,
  title = {Color Point Cloud Registration Based on Supervoxel Correspondence},
  author = {Yang, Yang and Chen, Weile and Wang, Muyi and Zhong, Dexing and Du, Shaoyi},
  date = {2020},
  journaltitle = {Ieee Access},
  volume = {8},
  pages = {7362--7372},
  publisher = {IEEE},
  doi = {10.1109/ACCESS.2020.2963987},
  url = {https://ieeexplore.ieee.org/abstract/document/8950119/},
  urldate = {2024-06-03},
  file = {C:\Users\18317\Zotero\storage\YZEIKX8R\Yang et al. - 2020 - Color point cloud registration based on supervoxel.pdf}
}

@article{yangColorPointCloud2020a,
  title = {Color Point Cloud Registration Based on Supervoxel Correspondence},
  author = {Yang, Yang and Chen, Weile and Wang, Muyi and Zhong, Dexing and Du, Shaoyi},
  date = {2020},
  journaltitle = {Ieee Access},
  volume = {8},
  pages = {7362--7372},
  publisher = {IEEE},
  doi = {10.1109/ACCESS.2020.2963987},
  url = {https://ieeexplore.ieee.org/abstract/document/8950119/},
  urldate = {2024-06-05},
  file = {C:\Users\18317\DevSpace\DocHub\essay\zotero\attachments\raw\Yang et al_2020_Color point cloud registration based on supervoxel correspondence2.pdf}
}

@article{yangTeaserFastCertifiable2020,
  title = {Teaser: {{Fast}} and Certifiable Point Cloud Registration},
  shorttitle = {Teaser},
  author = {Yang, Heng and Shi, Jingnan and Carlone, Luca},
  date = {2020},
  journaltitle = {IEEE Transactions on Robotics},
  volume = {37},
  number = {2},
  pages = {314--333},
  publisher = {IEEE},
  doi = {10.1109/TRO.2020.3033695},
  url = {https://ieeexplore.ieee.org/abstract/document/9286491/},
  urldate = {2024-05-21},
  file = {C:\Users\18317\DevSpace\DocHub\essay\zotero\attachments\GICP\Yang et al_2020_Teaser.pdf}
}

@online{yeMathematicalSupplementTexttt2023,
  title = {Mathematical {{Supplement}} for the \$\textbackslash texttt\{gsplat\}\$ {{Library}}},
  author = {Ye, Vickie and Kanazawa, Angjoo},
  date = {2023-12-04},
  eprint = {2312.02121},
  eprinttype = {arXiv},
  eprintclass = {cs, math},
  url = {http://arxiv.org/abs/2312.02121},
  urldate = {2024-06-29},
  abstract = {This report provides the mathematical details of the gsplat library, a modular toolbox for efficient differentiable Gaussian splatting, as proposed by Kerbl et al. It provides a self-contained reference for the computations involved in the forward and backward passes of differentiable Gaussian splatting. To facilitate practical usage and development, we provide a user friendly Python API that exposes each component of the forward and backward passes in rasterization at github.com/nerfstudio-project/gsplat .},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Graphics,Computer Science - Mathematical Software,Mathematics - Numerical Analysis},
  file = {C\:\\Users\\18317\\DevSpace\\DocHub\\essay\\zotero\\attachments\\gs_spalt\\Ye_Kanazawa_2023_Mathematical Supplement for the $-texttt gsplat $ Library2.pdf;C\:\\Users\\18317\\Zotero\\storage\\FVVZUTYQ\\2312.html}
}

@inproceedings{yen-chenInerfInvertingNeural2021,
  title = {Inerf: {{Inverting}} Neural Radiance Fields for Pose Estimation},
  shorttitle = {Inerf},
  booktitle = {2021 {{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}} ({{IROS}})},
  author = {Yen-Chen, Lin and Florence, Pete and Barron, Jonathan T. and Rodriguez, Alberto and Isola, Phillip and Lin, Tsung-Yi},
  date = {2021},
  pages = {1323--1330},
  publisher = {IEEE},
  doi = {10.1109/IROS51168.2021.9636708},
  url = {https://ieeexplore.ieee.org/abstract/document/9636708/},
  urldate = {2024-09-13},
  file = {C:\Users\18317\DevSpace\DocHub\essay\zotero\attachments\gs_spalt\relate_works\Yen-Chen et al_2021_inerf.pdf}
}

@online{yiGaussianDreamerFastGeneration2024,
  title = {{{GaussianDreamer}}: {{Fast Generation}} from {{Text}} to {{3D Gaussians}} by {{Bridging 2D}} and {{3D Diffusion Models}}},
  shorttitle = {{{GaussianDreamer}}},
  author = {Yi, Taoran and Fang, Jiemin and Wang, Junjie and Wu, Guanjun and Xie, Lingxi and Zhang, Xiaopeng and Liu, Wenyu and Tian, Qi and Wang, Xinggang},
  date = {2024-05-13},
  eprint = {2310.08529},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2310.08529},
  urldate = {2024-09-03},
  abstract = {In recent times, the generation of 3D assets from text prompts has shown impressive results. Both 2D and 3D diffusion models can help generate decent 3D objects based on prompts. 3D diffusion models have good 3D consistency, but their quality and generalization are limited as trainable 3D data is expensive and hard to obtain. 2D diffusion models enjoy strong abilities of generalization and fine generation, but 3D consistency is hard to guarantee. This paper attempts to bridge the power from the two types of diffusion models via the recent explicit and efficient 3D Gaussian splatting representation. A fast 3D object generation framework, named as GaussianDreamer, is proposed, where the 3D diffusion model provides priors for initialization and the 2D diffusion model enriches the geometry and appearance. Operations of noisy point growing and color perturbation are introduced to enhance the initialized Gaussians. Our GaussianDreamer can generate a high-quality 3D instance or 3D avatar within 15 minutes on one GPU, much faster than previous methods, while the generated instances can be directly rendered in real time. Demos and code are available at https://taoranyi.com/gaussiandreamer/.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Graphics},
  file = {C\:\\Users\\18317\\DevSpace\\DocHub\\essay\\zotero\\attachments\\gs_spalt\\relate_works\\Yi et al_2024_GaussianDreamer.pdf;C\:\\Users\\18317\\Zotero\\storage\\8ZCLAKS8\\2310.html}
}

@online{yuGaussianOpacityFields2024,
  title = {Gaussian {{Opacity Fields}}: {{Efficient}} and {{Compact Surface Reconstruction}} in {{Unbounded Scenes}}},
  shorttitle = {Gaussian {{Opacity Fields}}},
  author = {Yu, Zehao and Sattler, Torsten and Geiger, Andreas},
  date = {2024-04-16},
  eprint = {2404.10772},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2404.10772},
  urldate = {2024-07-17},
  abstract = {Recently, 3D Gaussian Splatting (3DGS) has demonstrated impressive novel view synthesis results, while allowing the rendering of high-resolution images in real-time. However, leveraging 3D Gaussians for surface reconstruction poses significant challenges due to the explicit and disconnected nature of 3D Gaussians. In this work, we present Gaussian Opacity Fields (GOF), a novel approach for efficient, high-quality, and compact surface reconstruction in unbounded scenes. Our GOF is derived from ray-tracing-based volume rendering of 3D Gaussians, enabling direct geometry extraction from 3D Gaussians by identifying its levelset, without resorting to Poisson reconstruction or TSDF fusion as in previous work. We approximate the surface normal of Gaussians as the normal of the ray-Gaussian intersection plane, enabling the application of regularization that significantly enhances geometry. Furthermore, we develop an efficient geometry extraction method utilizing marching tetrahedra, where the tetrahedral grids are induced from 3D Gaussians and thus adapt to the scene's complexity. Our evaluations reveal that GOF surpasses existing 3DGS-based methods in surface reconstruction and novel view synthesis. Further, it compares favorably to, or even outperforms, neural implicit methods in both quality and speed.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\18317\\DevSpace\\DocHub\\essay\\zotero\\attachments\\gs_spalt\\Yu et al_2024_Gaussian Opacity Fields2.pdf;C\:\\Users\\18317\\Zotero\\storage\\7V8MHQBA\\2404.html}
}

@online{yugayGaussianSLAMPhotorealisticDense2024,
  title = {Gaussian-{{SLAM}}: {{Photo-realistic Dense SLAM}} with {{Gaussian Splatting}}},
  shorttitle = {Gaussian-{{SLAM}}},
  author = {Yugay, Vladimir and Li, Yue and Gevers, Theo and Oswald, Martin R.},
  date = {2024-03-22},
  eprint = {2312.10070},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2312.10070},
  urldate = {2024-06-09},
  abstract = {We present a dense simultaneous localization and mapping (SLAM) method that uses 3D Gaussians as a scene representation. Our approach enables interactive-time reconstruction and photo-realistic rendering from real-world single-camera RGBD videos. To this end, we propose a novel effective strategy for seeding new Gaussians for newly explored areas and their effective online optimization that is independent of the scene size and thus scalable to larger scenes. This is achieved by organizing the scene into sub-maps which are independently optimized and do not need to be kept in memory. We further accomplish frame-to-model camera tracking by minimizing photometric and geometric losses between the input and rendered frames. The Gaussian representation allows for high-quality photo-realistic real-time rendering of real-world scenes. Evaluation on synthetic and real-world datasets demonstrates competitive or superior performance in mapping, tracking, and rendering compared to existing neural dense SLAM methods.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Robotics},
  file = {C\:\\Users\\18317\\DevSpace\\DocHub\\essay\\zotero\\attachments\\gs_spalt\\Yugay et al_2024_Gaussian-SLAM2.pdf;C\:\\Users\\18317\\Zotero\\storage\\9J38D593\\2312.html}
}

@inproceedings{yuPlenoctreesRealtimeRendering2021,
  title = {Plenoctrees for Real-Time Rendering of Neural Radiance Fields},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF International Conference}} on {{Computer Vision}}},
  author = {Yu, Alex and Li, Ruilong and Tancik, Matthew and Li, Hao and Ng, Ren and Kanazawa, Angjoo},
  date = {2021},
  pages = {5752--5761},
  url = {http://openaccess.thecvf.com/content/ICCV2021/html/Yu_PlenOctrees_for_Real-Time_Rendering_of_Neural_Radiance_Fields_ICCV_2021_paper.html},
  urldate = {2024-09-13},
  keywords = {⛔ No DOI found},
  file = {C:\Users\18317\DevSpace\DocHub\essay\zotero\attachments\gs_spalt\relate_works\Yu et al_2021_Plenoctrees for real-time rendering of neural radiance fields.pdf}
}

@inproceedings{yuPlenoctreesRealtimeRendering2021a,
  title = {Plenoctrees for Real-Time Rendering of Neural Radiance Fields},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF International Conference}} on {{Computer Vision}}},
  author = {Yu, Alex and Li, Ruilong and Tancik, Matthew and Li, Hao and Ng, Ren and Kanazawa, Angjoo},
  date = {2021},
  pages = {5752--5761},
  url = {http://openaccess.thecvf.com/content/ICCV2021/html/Yu_PlenOctrees_for_Real-Time_Rendering_of_Neural_Radiance_Fields_ICCV_2021_paper.html},
  urldate = {2024-09-26},
  keywords = {⛔ No DOI found},
  file = {C:\Users\18317\DevSpace\DocHub\essay\zotero\attachments\gs_spalt\relate_works\Yu et al_2021_Plenoctrees for real-time rendering of neural radiance fields2.pdf}
}

@article{zhangNeuralGuidedVisual2021,
  title = {Neural Guided Visual Slam System with {{Laplacian}} of {{Gaussian}} Operator},
  author = {Zhang, Ge and Yan, Xiaoqiang and Xu, Yulong and Ye, Yangdong},
  date = {2021-04},
  journaltitle = {IET Computer Vision},
  volume = {15},
  number = {3},
  pages = {181--196},
  publisher = {{Institution of Engineering and Technology (IET)}},
  issn = {1751-9632, 1751-9640},
  doi = {10.1049/cvi2.12022},
  url = {https://onlinelibrary.wiley.com/doi/10.1049/cvi2.12022},
  urldate = {2024-09-17},
  langid = {english},
  file = {C:\Users\18317\DevSpace\DocHub\essay\zotero\attachments\raw\Zhang et al_2021_Neural guided visual slam system with Laplacian of Gaussian operator.pdf}
}

@article{zhangRGBDCameraBased2021,
  title = {An {{RGB-D}} Camera Based Visual Positioning System for Assistive Navigation by a Robotic Navigation Aid},
  author = {Zhang, He and Jin, Lingqiu and Ye, Cang},
  date = {2021},
  journaltitle = {IEEE/CAA Journal of Automatica Sinica},
  volume = {8},
  number = {8},
  pages = {1389--1400},
  publisher = {IEEE},
  doi = {10.1109/JAS.2021.1004084},
  url = {https://ieeexplore.ieee.org/abstract/document/9459587/},
  urldate = {2024-05-22},
  file = {C\:\\Users\\18317\\DevSpace\\DocHub\\essay\\zotero\\attachments\\raw\\Zhang et al_2021_An RGB-D camera based visual positioning system for assistive navigation by a.pdf;C\:\\Users\\18317\\Zotero\\storage\\S8I3K9V8\\JAS.2021.html}
}

@article{zhongRobustRigidRegistration2021,
  title = {A Robust Rigid Registration Framework of {{3D}} Indoor Scene Point Clouds Based on {{RGB-D}} Information},
  author = {Zhong, Saishang and Guo, Mingqiang and Lv, Ruina and Chen, Jianguo and Xie, Zhong and Liu, Zheng},
  date = {2021},
  journaltitle = {Remote Sensing},
  volume = {13},
  number = {23},
  pages = {4755},
  publisher = {MDPI},
  doi = {10.3390/rs13234755},
  url = {https://www.mdpi.com/2072-4292/13/23/4755},
  urldate = {2024-05-22},
  file = {C\:\\Users\\18317\\DevSpace\\DocHub\\essay\\zotero\\attachments\\raw\\Zhong et al_2021_A robust rigid registration framework of 3D indoor scene point clouds based on.pdf;C\:\\Users\\18317\\Zotero\\storage\\342RPF2H\\4755.html}
}

@online{zhouOpen3DModernLibrary2018,
  title = {{{Open3D}}: {{A Modern Library}} for {{3D Data Processing}}},
  shorttitle = {{{Open3D}}},
  author = {Zhou, Qian-Yi and Park, Jaesik and Koltun, Vladlen},
  date = {2018-01-29},
  eprint = {1801.09847},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1801.09847},
  urldate = {2024-08-20},
  abstract = {Open3D is an open-source library that supports rapid development of software that deals with 3D data. The Open3D frontend exposes a set of carefully selected data structures and algorithms in both C++ and Python. The backend is highly optimized and is set up for parallelization. Open3D was developed from a clean slate with a small and carefully considered set of dependencies. It can be set up on different platforms and compiled from source with minimal effort. The code is clean, consistently styled, and maintained via a clear code review mechanism. Open3D has been used in a number of published research projects and is actively deployed in the cloud. We welcome contributions from the open-source community.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Graphics,Computer Science - Robotics},
  file = {C\:\\Users\\18317\\DevSpace\\DocHub\\essay\\zotero\\attachments\\gs_spalt\\experiments\\Zhou et al_2018_Open3D.pdf;C\:\\Users\\18317\\Zotero\\storage\\HZJSVII4\\1801.html}
}

@inproceedings{zhouUnsupervisedLearningDepth2017,
  title = {Unsupervised Learning of Depth and Ego-Motion from Video},
  booktitle = {Proceedings of the {{IEEE}} Conference on Computer Vision and Pattern Recognition},
  author = {Zhou, Tinghui and Brown, Matthew and Snavely, Noah and Lowe, David G.},
  date = {2017},
  pages = {1851--1858},
  url = {http://openaccess.thecvf.com/content_cvpr_2017/html/Zhou_Unsupervised_Learning_of_CVPR_2017_paper.html},
  urldate = {2024-09-26},
  keywords = {⛔ No DOI found},
  file = {C:\Users\18317\DevSpace\DocHub\essay\zotero\attachments\gs_spalt\relate_works\Zhou et al_2017_Unsupervised learning of depth and ego-motion from video.pdf}
}

@inproceedings{zhuNICERSLAMNeuralImplicit2024,
  title = {{{NICER-SLAM}}: {{Neural Implicit Scene Encoding}} for {{RGB SLAM}}},
  shorttitle = {{{NICER-SLAM}}},
  booktitle = {2024 {{International Conference}} on {{3D Vision}} ({{3DV}})},
  author = {Zhu, Zihan and Peng, Songyou and Larsson, Viktor and Cui, Zhaopeng and Oswald, Martin R. and Geiger, Andreas and Pollefeys, Marc},
  date = {2024-03},
  pages = {42--52},
  issn = {2475-7888},
  doi = {10.1109/3DV62453.2024.00096},
  url = {https://ieeexplore.ieee.org/abstract/document/10550721},
  urldate = {2024-09-02},
  abstract = {Neural implicit representations have recently become popular in simultaneous localization and mapping (SLAM), especially in dense visual SLAM. However, existing works either rely on RGB-D sensors or require a separate monocular SLAM approach for camera tracking, and fail to produce high-fidelity 3D dense reconstructions. To address these shortcomings, we present NICER-SLAM, a dense RGB SLAM system that simultaneously optimizes for camera poses and a hierarchical neural implicit map representation, which also allows for high-quality novel view synthesis. To facilitate the optimization process for mapping, we integrate additional supervision signals including easy-to-obtain monocular geometric cues and optical flow, and also introduce a simple warping loss to further enforce geometric consistency. Moreover, to further boost performance in complex large-scale scenes, we also propose a local adaptive transformation from signed distance functions (SDFs) to density in the volume rendering equation. On multiple challenging indoor and outdoor datasets, NICER-SLAM demonstrates strong performance in dense mapping, novel view synthesis, and tracking, even competitive with recent RGB-D SLAM systems. Project page: https://nicer-slam.github.io/.},
  eventtitle = {2024 {{International Conference}} on {{3D Vision}} ({{3DV}})},
  keywords = {Cameras,NeRF,neural implicit representation,Pipelines,Rendering (computer graphics),Simultaneous localization and mapping,SLAM,Surface reconstruction,Three-dimensional displays,Tracking loops},
  file = {C:\Users\18317\Zotero\storage\VWX87T9S\10550721.html}
}

@inproceedings{zhuNicerslamNeuralImplicit2024a,
  title = {Nicer-Slam: {{Neural}} Implicit Scene Encoding for Rgb Slam},
  shorttitle = {Nicer-Slam},
  booktitle = {2024 {{International Conference}} on {{3D Vision}} ({{3DV}})},
  author = {Zhu, Zihan and Peng, Songyou and Larsson, Viktor and Cui, Zhaopeng and Oswald, Martin R. and Geiger, Andreas and Pollefeys, Marc},
  date = {2024},
  pages = {42--52},
  publisher = {IEEE},
  doi = {10.1109/3DV62453.2024.00096},
  url = {https://ieeexplore.ieee.org/abstract/document/10550721/},
  urldate = {2024-09-03},
  file = {C:\Users\18317\DevSpace\DocHub\essay\zotero\attachments\gs_spalt\relate_works\Zhu et al_2024_Nicer-slam.pdf}
}

@inproceedings{zhuNiceslamNeuralImplicit2022,
  title = {Nice-Slam: {{Neural}} Implicit Scalable Encoding for Slam},
  shorttitle = {Nice-Slam},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF}} Conference on Computer Vision and Pattern Recognition},
  author = {Zhu, Zihan and Peng, Songyou and Larsson, Viktor and Xu, Weiwei and Bao, Hujun and Cui, Zhaopeng and Oswald, Martin R. and Pollefeys, Marc},
  date = {2022},
  pages = {12786--12796},
  url = {http://openaccess.thecvf.com/content/CVPR2022/html/Zhu_NICE-SLAM_Neural_Implicit_Scalable_Encoding_for_SLAM_CVPR_2022_paper.html},
  urldate = {2024-09-03},
  keywords = {⛔ No DOI found},
  file = {C:\Users\18317\DevSpace\DocHub\essay\zotero\attachments\gs_spalt\relate_works\Zhu et al_2022_Nice-slam.pdf}
}

@article{zwickerEWASplatting2002,
  title = {{{EWA}} Splatting},
  author = {Zwicker, Matthias and Pfister, Hanspeter and Van Baar, Jeroen and Gross, Markus},
  date = {2002},
  journaltitle = {IEEE Transactions on Visualization and Computer Graphics},
  volume = {8},
  number = {3},
  pages = {223--238},
  publisher = {IEEE},
  doi = {10.1109/TVCG.2002.1021576},
  url = {https://ieeexplore.ieee.org/abstract/document/1021576/},
  urldate = {2024-07-20},
  file = {C:\Users\18317\DevSpace\DocHub\essay\zotero\attachments\gs_spalt\Zwicker et al_2002_EWA splatting.pdf}
}

@article{zwickerEWASplatting2002a,
  title = {{{EWA}} Splatting},
  author = {Zwicker, Matthias and Pfister, Hanspeter and Van Baar, Jeroen and Gross, Markus},
  date = {2002},
  journaltitle = {IEEE Transactions on Visualization and Computer Graphics},
  volume = {8},
  number = {3},
  pages = {223--238},
  publisher = {IEEE},
  doi = {10.1109/TVCG.2002.1021576},
  url = {https://ieeexplore.ieee.org/abstract/document/1021576/},
  urldate = {2024-07-20},
  file = {C:\Users\18317\DevSpace\DocHub\essay\zotero\attachments\gs_spalt\method\Zwicker et al_2002_EWA splatting.pdf}
}
